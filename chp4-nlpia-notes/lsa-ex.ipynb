{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA utilises a common technique for dimension reduction, namely Singular Value Decomposition (SVD).\n",
    "<br>\n",
    "SVD decomposes a matrix into three square matrices - one of which is diagonal.\n",
    "* Applications - Given that SVD utilises matrix inversion as its core transformation, it allows for many real word uses within data science including behaviour-based recommendation engines that run alongside content-based NLP recommendation engines\n",
    "* SVD purpose - Allows Truncation of those matrices (ignore some rows/columns) before multiplying them back together, which reduces the number of dimensions one has to deal with in our vector space model\n",
    "* Modified transformation - Truncated matrices can give a slightly better TF-IDF matrix representation then the one started with. The new representation of documents contains the essence (latent semantics) of those documents. It captures the essence of a dataset and ignores the noise, making it useful for applications the require compression\n",
    "* Summary - SVD used in NLP is seen as LSA, which uncovers the meanings of words that is hidden and urging to be explored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Technical explanation behind LSA:***\n",
    "<br>\n",
    "<br>\n",
    "LSA is a mathematical technique for finding the 'best' way to linearly transform (rotate and stretch) any set of NLP vectors e.g. BOW or TF-IDF vectors.\n",
    "* Optimisation - The ideal method for different applications is to line up the axes (dimensions) in the new vectors with the greatest variance in the word frequencies\n",
    "* Filtering - We can then eliminate those dimensions in the new vector space that do not contribute much to the variance in the vectors from document to document\n",
    "* Related concept - **Principal Component Analysis** (PCA) on TF-IDF vectors is identical to LSA on natural language documents, which is useful for problems and areas involving *feature engineering*\n",
    "* Computation - LSA uses SVD to find the combinations of words that are responsible (together), for the greatest variation in the data. As mentioned earlier, we rotate TF-IDF vectors so that the new dimensions (basis vectors) of our rotated vectors all allign with these maximum variance directions. The basis vectors comprise of the axes of our new vector space, which are analogous to our new vector space. Each of the dimensions becomes a combination of word frequencies rather than a single word frequency.\n",
    "* Interpretation - We can think of the output vectors as the weighted combinations of words that make up various 'topics' used throughout a given corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine/programme doesn't know what the combinations of words means, it just identifies that they go together.\n",
    "* Words together - Seeing words like 'dog', 'cat' and 'love' together frequently means the programme will cluster them terms together under a topic\n",
    "* Topic identification - The programme doesn't automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}