{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is the core algorithm behind LSA.\n",
    "<br>\n",
    "As an instance, start with a corpus containing 11 documents and a vocabulary of 6 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 263/263 [00:00<00:00, 199367.78it/s]\n"
    }
   ],
   "source": [
    "from nlpia.book.examples.ch04_catdog_lsa_sorted import lsa_models, prettify_tdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 263/263 [00:00<00:00, 169213.37it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   cat dog apple lion nyc love                                             text\n0              1        1                                 NYC is the Big Apple.\n1              1        1                        NYC is known as the Big Apple.\n2                       1    1                                      I love NYC!\n3              1        1           I wore a hat to the Big Apple party in NYC.\n4              1        1                       Come to NYC. See the Big Apple!\n5              1                             Manhattan is called the Big Apple.\n6    1                                  New York is a big city for a small cat.\n7    1              1           The lion, a big cat, is the king of the jungle.\n8    1                       1                               I love my pet cat.\n9                       1    1                      I love New York City (NYC).\n10   1   1                                              Your dog chased my cat.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat</th>\n      <th>dog</th>\n      <th>apple</th>\n      <th>lion</th>\n      <th>nyc</th>\n      <th>love</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>NYC is the Big Apple.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>NYC is known as the Big Apple.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td>1</td>\n      <td>I love NYC!</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>I wore a hat to the Big Apple party in NYC.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td>Come to NYC. See the Big Apple!</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Manhattan is called the Big Apple.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>New York is a big city for a small cat.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td>The lion, a big cat, is the king of the jungle.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td>I love my pet cat.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td>1</td>\n      <td>I love New York City (NYC).</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>Your dog chased my cat.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "bow_svd, tfidf_svd = lsa_models()\n",
    "prettify_tdm(**bow_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix shown is a document-term matrix (dtm) where each row is a vector of the BOW for a document.\n",
    "* Interpretation - The sorting algorithm and the limited vocabulary created several identical BOW vectors (NYC, apple)\n",
    "* SVD operations - SVD should be able to notice this and allocate a topic to such pair of words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can use SVD on the term-document matrix (tdm) - the transposition of a dtm - where it can work on TF-IDF matrices or any other vector space model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       0  1  2  3  4  5  6  7  8  9  10\ncat    0  0  0  0  0  0  1  1  1  0   1\ndog    0  0  0  0  0  0  0  0  0  0   1\napple  1  1  0  1  1  1  0  0  0  0   0\nlion   0  0  0  0  0  0  0  1  0  0   0\nnyc    1  1  1  1  1  0  0  0  0  1   0\nlove   0  0  1  0  0  0  0  0  1  1   0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cat</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>dog</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>apple</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>lion</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>nyc</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>love</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tdm = bow_svd['tdm']\n",
    "tdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD is an algorithm for decomposing any matrix into three factors - three matrices that can be multiplied together to recreate the original matrix.\n",
    "* Purpose - The three matrix factors computed with SVD contain some convenient mathematical properties we can exploit for dimension reduction and LSA. LSA will be used to figure out topics (group of related words) need to be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether we run SVD on a word vector represention (BOW or TFIDF term-document matrices), SVD will find combinations of words that belong together \n",
    "* Process - SVD finds those co-occurring words by calculating the correlation between the columns (terms) of our term-document matrix\n",
    "* Computation - SVD simultaneously finds the correlation of term use between documents and the correlation of documents with each other, additionally computing the linear combinations of terms that have the greatest variation across the corpus\n",
    "* Filtering and dimensions reduction - We'll only keep those topics that retain the most information i.e. the most variance in our corpus\n",
    "* Transformation - SVD gives us the linear transformation (rotation) of our term-document vectors to convert those vectors into shorter topic vectors for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD will group together terms that have high correlation with each other (given they appear in the same documents together frequently) and also vary together a lot over the set of documents.\n",
    "<br>\n",
    "These linear combinations of words are seen as 'topics'.\n",
    "<br>\n",
    "These topics that turn our BOW/TF-IDF vectors into topic vectors that tell us the topics a document is about.\n",
    "<br>\n",
    "A topic vector provides a summarization or generalization of what the document is about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematical terms, SVD appears like this:\n",
    "<br>\n",
    "<br>\n",
    "$W_{mxn}$ &#8594; $U_{mxp}$$S_{pxp}$$V_{pxn}$$^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $m$ - The number of terms in one's vocabulary \n",
    "* $n$ - The number of documents in a corpus \n",
    "* $p$ - The number of topics in a corpus i.e. the number of words \n",
    "\n",
    "We want to eventually end up with fewer topics than words, so we can use those topic vectors (rows of topic-document matrix) as a reduced-dimension representation of the original TF-IDF vectors. \n",
    "<br>\n",
    "But currently in this example as a first stage, we retain all the dimensions in our matrices.\n",
    "<br>\n",
    "It's key to uncover what these three matrices (U, S and V) look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $U$ - left singular values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $U$ matrix contains the term-topic matrix that tells us about 'the company a word keeps' - the core matrix for semantic analysis in NLP.\n",
    "* **Left singular matrix** - The $U$ matrix is also known to be the 'left singular matrix' as it contains row vectors that should be multiplied by a matrix of column vectors from the left \n",
    "* Representation - $U$ is the cross-correlation between words and topics based on word co-occurrence in the same document\n",
    "\n",
    "$U$ is a square matrix until we start truncating it (deleting columns). \n",
    "* Dimensions - $U$ contains the same number of rows and columns as we have words in our vocabulary ($m$): 6. Given we haven't truncated this matrix yet, the number of topics ($p$) remains 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          0     1     2     3     4     5\ncat   -0.04  0.83 -0.38  0.00  0.11 -0.38\ndog   -0.00  0.21 -0.18 -0.71 -0.39  0.52\napple -0.62 -0.21 -0.51  0.00  0.49  0.27\nlion  -0.00  0.21 -0.18  0.71 -0.39  0.52\nnyc   -0.75 -0.00  0.24 -0.00 -0.52 -0.32\nlove  -0.22  0.42  0.69  0.00  0.41  0.37",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>cat</th>\n      <td>-0.04</td>\n      <td>0.83</td>\n      <td>-0.38</td>\n      <td>0.00</td>\n      <td>0.11</td>\n      <td>-0.38</td>\n    </tr>\n    <tr>\n      <th>dog</th>\n      <td>-0.00</td>\n      <td>0.21</td>\n      <td>-0.18</td>\n      <td>-0.71</td>\n      <td>-0.39</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>apple</th>\n      <td>-0.62</td>\n      <td>-0.21</td>\n      <td>-0.51</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>lion</th>\n      <td>-0.00</td>\n      <td>0.21</td>\n      <td>-0.18</td>\n      <td>0.71</td>\n      <td>-0.39</td>\n      <td>0.52</td>\n    </tr>\n    <tr>\n      <th>nyc</th>\n      <td>-0.75</td>\n      <td>-0.00</td>\n      <td>0.24</td>\n      <td>-0.00</td>\n      <td>-0.52</td>\n      <td>-0.32</td>\n    </tr>\n    <tr>\n      <th>love</th>\n      <td>-0.22</td>\n      <td>0.42</td>\n      <td>0.69</td>\n      <td>0.00</td>\n      <td>0.41</td>\n      <td>0.37</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "U, s, Vt = np.linalg.svd(tdm)\n",
    "pd.DataFrame(U, index=tdm.index).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $U$ matrix contains all the topic vectors for each word in our corpus as columns.\n",
    "* Conversion - Such matrix can be used as a transformation to convert a word-document vector (TF-IDF/BOW vector) into a topic-document vector\n",
    "* Weight computation - We multiply our topic vector $U$ matrix by any word-document column vector to get a new topic-document vector, given that the weights/scores in each cell of the $U$ matrix represent how important each word is to each topic \n",
    "\n",
    "In essence, the $U$ matrix in terms of SVD provides us with everything needed to map word frequencies to topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $S$ - singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sigma or $S$ matrix contains the topic 'singular values' in a square diagonal matrix \n",
    "* Singular values - such values tell us how much information is captured by each dimension in our new semantic (topic) vector space\n",
    "* Diagonal matrix - contains only nonzero values along the diagonal from the upper left to the right. Everywhere else will contain zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([3.1, 2.2, 1.8, 1. , 0.8, 0.5])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "s.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros((len(U), len(Vt)))\n",
    "pd.np.fill_diagonal(S, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     0    1    2    3    4    5    6    7    8    9   10\n0  3.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n1  0.0  2.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n2  0.0  0.0  1.8  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n4  0.0  0.0  0.0  0.0  0.8  0.0  0.0  0.0  0.0  0.0  0.0\n5  0.0  0.0  0.0  0.0  0.0  0.5  0.0  0.0  0.0  0.0  0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>2.2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pd.DataFrame(S).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the $U$ matrix, our $S$ matrix for the 6 word/topic corpus contains 6 rows ($p$), but it contains many more columns ($n$) filled with zeros.\n",
    "* Process - It needs a column for every document so we can multiply it by $V^T$ (document-document matrix). As we haven't yet reduced the dimensionality via truncating this diagonal matrix, we have as many topics ($p$) as we have terms in our vocabulary ($m$), which is six in this case\n",
    "* Dimension construction - the dimensions (topics) are constructed such that the first dimension contains the most information ('explained variance') about our corpus\n",
    "* Truncating - When it comes to truncating the topic model, we can begin zeroing out the dimensions at the lower right and work our way up to the left. We can stop zeroing out these sigular values when the error in our topic model starts to contribute significantly to the overall NLP pipeline error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $V^T$ - right singular vectors \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $V^T$ matrix contains the 'right singular vectors' as the columns of the document-document matrix.\n",
    "* Purpose - The $V^T$ matrix gives us the shared meaning between documents, since it measures how often documents use the same topics in our new semantic model of documents \n",
    "* Representation - It has the same number of rows ($p$) and columns as we have documents in our small corpus, which in this case is 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       0     1     2     3     4     5     6     7     8     9    10\n0  -0.44 -0.44 -0.31 -0.44 -0.44 -0.20 -0.01 -0.01 -0.08 -0.31 -0.01\n1  -0.09 -0.09  0.19 -0.09 -0.09 -0.09  0.37  0.47  0.56  0.19  0.47\n2  -0.16 -0.16  0.52 -0.16 -0.16 -0.29 -0.22 -0.32  0.17  0.52 -0.32\n3   0.00 -0.00 -0.00 -0.00 -0.00  0.00 -0.00  0.71  0.00 -0.00 -0.71\n4  -0.04 -0.04 -0.14 -0.04 -0.04  0.58  0.13 -0.33  0.62 -0.14 -0.33\n5  -0.09 -0.09  0.10 -0.09 -0.09  0.51 -0.73  0.27 -0.01  0.10  0.27\n6  -0.57  0.21  0.11  0.33 -0.31  0.34  0.34 -0.00 -0.34  0.23  0.00\n7  -0.32  0.47  0.25 -0.63  0.41  0.07  0.07  0.00 -0.07 -0.18  0.00\n8  -0.50  0.29 -0.20  0.41  0.16 -0.37 -0.37 -0.00  0.37 -0.17  0.00\n9  -0.15 -0.15 -0.59 -0.15  0.42  0.04  0.04  0.00 -0.04  0.63 -0.00\n10 -0.26 -0.62  0.33  0.24  0.54  0.09  0.09  0.00 -0.09 -0.23 -0.00",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.44</td>\n      <td>-0.44</td>\n      <td>-0.31</td>\n      <td>-0.44</td>\n      <td>-0.44</td>\n      <td>-0.20</td>\n      <td>-0.01</td>\n      <td>-0.01</td>\n      <td>-0.08</td>\n      <td>-0.31</td>\n      <td>-0.01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.09</td>\n      <td>-0.09</td>\n      <td>0.19</td>\n      <td>-0.09</td>\n      <td>-0.09</td>\n      <td>-0.09</td>\n      <td>0.37</td>\n      <td>0.47</td>\n      <td>0.56</td>\n      <td>0.19</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.16</td>\n      <td>-0.16</td>\n      <td>0.52</td>\n      <td>-0.16</td>\n      <td>-0.16</td>\n      <td>-0.29</td>\n      <td>-0.22</td>\n      <td>-0.32</td>\n      <td>0.17</td>\n      <td>0.52</td>\n      <td>-0.32</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00</td>\n      <td>-0.00</td>\n      <td>-0.00</td>\n      <td>-0.00</td>\n      <td>-0.00</td>\n      <td>0.00</td>\n      <td>-0.00</td>\n      <td>0.71</td>\n      <td>0.00</td>\n      <td>-0.00</td>\n      <td>-0.71</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.04</td>\n      <td>-0.04</td>\n      <td>-0.14</td>\n      <td>-0.04</td>\n      <td>-0.04</td>\n      <td>0.58</td>\n      <td>0.13</td>\n      <td>-0.33</td>\n      <td>0.62</td>\n      <td>-0.14</td>\n      <td>-0.33</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-0.09</td>\n      <td>-0.09</td>\n      <td>0.10</td>\n      <td>-0.09</td>\n      <td>-0.09</td>\n      <td>0.51</td>\n      <td>-0.73</td>\n      <td>0.27</td>\n      <td>-0.01</td>\n      <td>0.10</td>\n      <td>0.27</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.57</td>\n      <td>0.21</td>\n      <td>0.11</td>\n      <td>0.33</td>\n      <td>-0.31</td>\n      <td>0.34</td>\n      <td>0.34</td>\n      <td>-0.00</td>\n      <td>-0.34</td>\n      <td>0.23</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.32</td>\n      <td>0.47</td>\n      <td>0.25</td>\n      <td>-0.63</td>\n      <td>0.41</td>\n      <td>0.07</td>\n      <td>0.07</td>\n      <td>0.00</td>\n      <td>-0.07</td>\n      <td>-0.18</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-0.50</td>\n      <td>0.29</td>\n      <td>-0.20</td>\n      <td>0.41</td>\n      <td>0.16</td>\n      <td>-0.37</td>\n      <td>-0.37</td>\n      <td>-0.00</td>\n      <td>0.37</td>\n      <td>-0.17</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-0.15</td>\n      <td>-0.15</td>\n      <td>-0.59</td>\n      <td>-0.15</td>\n      <td>0.42</td>\n      <td>0.04</td>\n      <td>0.04</td>\n      <td>0.00</td>\n      <td>-0.04</td>\n      <td>0.63</td>\n      <td>-0.00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-0.26</td>\n      <td>-0.62</td>\n      <td>0.33</td>\n      <td>0.24</td>\n      <td>0.54</td>\n      <td>0.09</td>\n      <td>0.09</td>\n      <td>0.00</td>\n      <td>-0.09</td>\n      <td>-0.23</td>\n      <td>-0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "pd.DataFrame(Vt).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the $S$ matrix, we can ignore the $V^T$ matrix whenever we're transforming new word-document vectors into our topic vector space.\n",
    "<br>\n",
    "The $V^T$ matrix may only be used once more to check the accuracy of topic vector for recreating the original word-document vectors that we used to 'train' it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform the SVD linear algebra directly, the matrix needs to be in the format (transposed if needed) consisting of a term-document matrix.\n",
    "<br>\n",
    "ML packages such as `sklearn` require a document-term matrix in terms of a training set instead, which in this case would mean:\n",
    "* Rows - Each row in the sample-feature matrix for a ML sample is a document (textual data like SMS messages, reviews, emails etc.)\n",
    "* Columns - Each column represents a word or feature (tokens) of those documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above notes, we have a way to constrauct a topic model - a way to transform word frequency vectors into topic weight vectors.\n",
    "<br>\n",
    "Although, given we have just as many topics as words, our vector space model has just as many dimensions as the original BOW vectors.\n",
    "<br>\n",
    "Carrying out this process means constructing some new words and calling them 'topics' as they each combine words together in various ratios.\n",
    "<br>\n",
    "We maintain the $U$ matrix given that it's already arranged so that the most important topics (with the largest singular values) are on the left - hence we can ignore the $S$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin this truncation process and in effect reducing the number of dimensions, we can loop off columns on the right-hand side of the $U$ matrix.\n",
    "* Accuracy/optimisation - To decide how many topics will be enough to capture the essence of a document, we need to measure the accuracy of LSA, which can be performed by indentifying how accurately we can recreate a term-document matrix from a topic-document  matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = [] \n",
    "for numdim in range(len(s), 0, -1):\n",
    "    S[numdim -1, numdim - 1] = 0 \n",
    "    reconstructed_tdm = U.dot(S).dot(Vt)\n",
    "    err.append(np.sqrt(((reconstructed_tdm - tdm).values.flatten() ** 2).sum() / np.product(tdm.shape)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.06, 0.12, 0.17, 0.28, 0.39, 0.55])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "np.array(err).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen among the elements in the array, when reconstructing a term-document matrix for our 11 documents using the singular vectors, the more we truncate, the more the error grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/lsa-tdm-accuracy-dimension-reduction.png\" alt=\"LSA model accuracy shown to be decreasing given such tdm reconstruction as the number of dimensions are ignored/removed\" width=\"400\" height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 NLPIA Lane, Howard and Hapke (2019) chp 4.3.5 pp. 330 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 illustrates such phenomena where accuracy drops as one excludes more and more dimensions in a given topic model.\n",
    "* Vector representation - The accuracy drop is quite similar between either BOW/TF-IDF vectors used for the model, although TF-IDF vectors will perform slightly better if we plan to retain only a few topics in such a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD algorithm behind LSA 'notices' if words are always used together and puts them together in a topic - exploiting a gain of few dimensions at no extra cost. \n",
    "* Other considerations - Even if a topic model isn't going to be used in a pipeline, LSA (SVD) can be a great way to compress one's word-document matrices and identify potential compound words or n-grams for a given pipeline"
   ]
  }
 ]
}