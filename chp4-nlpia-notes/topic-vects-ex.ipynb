{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding meaning in words counts (semantic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic analysis** emphasises the use of machines/programmes to understand the 'meaning' of words.\n",
    "<br>\n",
    "A reminder of TF-IDF word vectors scores for n-grams is that the application is useful for searching text if the exact words/n-grams that are to be subsequently search are known. \n",
    "<br>\n",
    "NLP applications in the past have found such algorithms for revealing the meaning of word combinations and computing vectors to represent this meaning - ***latent semantic analysis (LSA)***. Utilising this tool not only represents the meaning of words as vectors, but can also be used to represent the meaning of entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning more about ***semantic/topic vectors***, we can use weighted frequency scores from TFIDF vectors to compute the topic 'scores' that make up the dimensions of our topic vectors. The idea is to use the correlation of normalized term frequencies with each other to group words together in topics to define the dimensions of our new topic vectors.\n",
    "<br>\n",
    "Such methods make it possible to utilise interesting applications e.g. making it possible to search for documents based on their meaning - ***semantic search***. At times, semantic search returns search results that are much better than keyword search (TF-IDF search). It can return documents that are exactly what the user is searching for, even when the user can't think of the right words to put in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic vectors help us identify the words or n-grams that best represent the subject (topic) of a statement, document or corpus (collection of documents). Given this vector representation of words along with their *relative* importance, you can provide someone with the most meaningful words for a document - a set of keywords that summarizes its meaning.\n",
    "<br>\n",
    "Semantic vectors enable the possibility to compare any two statements/documents and tell how 'close' they are in *meaning* to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations (linear) of words that make up the dimensions of our topic vectors are powerful representations of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts to topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to score the meanings and topics the words are used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectors and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any word vector representation such as TF-IDF count the exact spellings of terms in a document. \n",
    "<br>\n",
    "As a reminder, texts that restate the same meaning will have completely different TF-IDF vector representations if they spell things differently or use different words. Such cases can confuse search engines and also document similarity comparisons relying on token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization approach kept similarly *spelled* words together within an analysis, but not necessarily words with similar meanings - failing to pair up most synonyms.\n",
    "<br>\n",
    "This is challenging as synonyms differ in more ways than just word endings that lemmatization and stemming deal with.\n",
    "<br>\n",
    "Sometimes, lemmatization and stemming can actually mistakenly group together antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, two chunks of text that talk about the same thing but use different words will not be 'close' to each other in our lemmatized TF-IDF vector space model.\n",
    "<br> \n",
    "An instance might be that the TF-IDF vector for one chapter in a book about NLP may not be close at all to similar-meaning passages in university textbooks about latent semantic indexing.\n",
    "<br>\n",
    "Generally, the NLP book might use more modernised jargon/terms than the university textbook, where university researchers use more consistent and rigorous language within textbooks/lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to search for a more optimal method to extract additional information and meaning from word statistics i.e. a better estimate of what the words in a document 'signify'. \n",
    "<br>\n",
    "Also, being wary of a better estimate of what the words in a document 'signify', all the while trying to understand what the combination of words *means* in a certain document. This would mean we'd like to represent that meaning with a vector that's like a TF-IDF vector, yet more compact and meanigful.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come up with two terms to describe the context:\n",
    "<br>\n",
    "1. **word-topic vectors** - Compact meaning vectors \n",
    "<br>\n",
    "2. **document-topic** - Document meaning vectors \n",
    "<br>\n",
    "<br>\n",
    "In any case, either of these can be called **topic vectors**\n",
    "<br>\n",
    "Topic vectors can be compact or as expansive (high dimensions) as desired. LSA topic vectors can have as little as one dimension or thousands of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mathematical operations between topic vectors (addition and subtraction) mean more than they did with TF-IDF vectors.\n",
    "<br>\n",
    "The distances between topic vectors is useful for things like clustering documents or semantic search (including search by semantics) - whereas doing TF-IDF topic modelling could only cluster and search using keywords via TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these computations are completed, we'll have one document-topic vector for each document in the corpus - which also usually translates to not having to reprocess the entire corpus to compute a new topic vector for a new document or phrase.\n",
    "<br>\n",
    "We'll have a topic vector for each word in our lexicon (vocabulary), where we can use/compute these word topic vectors for any new document by adding up all its word topic vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical representations of the semantics of words/sentences can be tricky.\n",
    "<br>\n",
    "Given languages like English have multiple diaclects and different interpretations of the same words - the concept of words with multiple meanings is known as ***polysemy***:\n",
    "* ***polysemy*** - The existence of words and phrases with more than one meaning \n",
    "\n",
    "Some ways polysemy can affect the semantics of word/statements. LSA actually manages handling these situations for us:\n",
    "* ***Homonyms*** - Words with the same spelling and pronounciation but different meanings e.g Bat (baseball bat and animal)\n",
    "* ***Zeugma*** - Use of two meanings of a word simultaneously in the same sentence e.g. I held **her hand** and **my tongue**\n",
    "\n",
    "LSA also deals with some of the challenges of polysemy in a voice interface (chatbot) that one can talk to, like Alexa or Siri:\n",
    "* ***Homographs*** — Words spelled the same, but with different pronunciations and meanings e.g. Bass (type of fish OR low deep voice)\n",
    "* ***Homophones*** - Words with the same pronunciation, but different spellings and meanings (an NLP challenge with voice interfaces) e.g. Blew and Blue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have a TF-IDF vector for a certain document and we want to convert that to a topic vector - think about how much each contributes to such named topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s say we're processing some sentences about pets in Central Park in New York City (NYC). We can create three topics: one about pets, one about animals, and another about cities. \n",
    "<br>\n",
    "Call these topics 'petness', 'animalness', and 'cityness'. So your 'petness' topic about pets will score words like 'cat' and 'dog' significantly, but probably ignore words like 'NYC' and 'apple'. The 'cityness' topic will ignore words like 'cat' and 'dog', but might give a little weight to 'apple', just because of the 'Big Apple' association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example would be where we 'train' the topic model as specified, and without using a computerised based solution (solely logic/common sense), we might come up with some weights like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tf-idf dict with randomised scores/weights\n",
    "topic = {} \n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['petness'] = (0.3 * tfidf['cat'] +\\\n",
    "                    0.3 * tfidf['dog'] +\\\n",
    "                    0 * tfidf['apple'] + 0 * tfidf['lion'] - 0.2 * tfidf['NYC'] +\\\n",
    "                    0.2 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['animalness'] = (0.1 * tfidf['cat'] +\\\n",
    "                       0.1 * tfidf['dog'] +\\\n",
    "                       0.1 * tfidf['apple'] + 0.5 * tfidf['lion'] + 0.1 * tfidf['NYC'] -\\\n",
    "                       0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['cityness'] = (0 * tfidf['cat'] -\\\n",
    "                     0.1 * tfidf['dog'] +\\\n",
    "                     0.2 * tfidf['apple'] - 0.1 * tfidf['lion'] + 0.5 * tfidf['NYC'] +\\\n",
    "                     0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual weights e.g like in 'petness' topic (0.3, 0.3, 0, 0, -0.2, 0.2) are multiplied by the randomised/imaginary tfidf values to create topic vectors for this imaginary/random document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we added up word frequencies that might be indicators of each topics.\n",
    "<br>\n",
    "We weighted the word frequencies (TF-IDF values) by how likely the word is associated with the topic. Similarly, for words that might be talking about something that is in some way the opposite of our topic, -ve weights would mean a disimilarity.\n",
    "<br>\n",
    "Running through this process, we get an illustration and simulation of how we teach a machine to think closer towards a human in term of natural language.\n",
    "<br>\n",
    "We arbitrarily chose to decompose our words and documents into only three topics ('petness', 'animalness' and 'cityness') - with a limited vocabulary of only six words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next high level step is to think how a human might decide mathematically which topics/words are connected, along with what specific weights such connections should have.\n",
    "* positve associations within topic - The terms 'cat' and 'dog' should have similar contributions to the 'petness' topic (weight of 0.3)\n",
    "* Negative associations within topic - The term 'NYC' should have a negative weight for the 'petness' topic. In general, city names, proper names, abbreviations and acronyms share little in common with words about pets.\n",
    "\n",
    "N.B - It's a good idea to keep a note of what 'sharing in common' means for words, ideally searching for something in a TF-IDF matrix that represents the meaning the words share in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***topic vectors weight computation:***\n",
    "<br>\n",
    "<br>\n",
    "Given the above signed weighting of words to produce topic vectors, given it was done manually, we decided to normalize the topic vectors via the $L^2$-norm.\n",
    "<br>\n",
    "Genuine applications of LSA normalizes topic vectors in a similar way as well via the $L^2$-norm. \n",
    "<br>\n",
    "$L^2$-norm is simply the traditional Euclidean distance/length that is conventionally seen in basic geometry. \n",
    "<br>\n",
    "It can be seen as the Pythagorean theorem solved for the length of the hypotenuse of a right triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships between words and topics can be reversed.\n",
    "<br>\n",
    "The 3 x 6 matrix of three topics can be transposed to produce topic weights for each word in our vocabulary (i.e. 6 x 3 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These vectors of weights would be our word vectors for our 6 words \n",
    "word_vector = {}\n",
    "word_vector['cat'] = 0.3 * topic['petness'] +\\\n",
    "                     0.1 * topic['animalness'] +\\\n",
    "                     0 * topic['cityness']\n",
    "word_vector['dog'] = 0.3 * topic['petness'] +\\\n",
    "                     0.1 * topic['animalness'] -\\\n",
    "                     0.1 * topic['cityness']\n",
    "word_vector['apple'] =  0 * topic['petness'] -\\\n",
    "                        0.1 * topic['animalness'] +\\\n",
    "                        0.2 * topic['cityness']  \n",
    "word_vector['lion'] = 0 * topic['petness'] +\\\n",
    "                      0.5 * topic['animalness'] -\\\n",
    "                      0.1 * topic['cityness']  \n",
    "word_vector['NYC'] =  -0.2 * topic['petness'] +\\\n",
    "                      0.1 * topic['animalness'] +\\\n",
    "                      0.5 * topic['cityness']\n",
    "word_vector['love'] = 0.2 * topic['petness'] -\\\n",
    "                      0.1 * topic['animalness'] +\\\n",
    "                      0.1 * topic['cityness']                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path().home()/'Desktop'/'nlp-map-project'/'chp4-nlpia-notes'/'img-vects'\n",
    "#os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img-vects/3d-vector-depiction-topic-vectors.png\" alt=\"3D vectors referring to six words and associated topic vectors\" width=\"400\" height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 NLPIA Lane, Howard and Hapke (2019) chp 4.1.3 pp. 287 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in Figure 1, these six topic vectors (one for each word) represent the meanings of our six words as 3D vectors.\n",
    "* Counting up occurrences of these six words and multiplying them by our weights retrieves the 3D topic vector for any document\n",
    "* Illustration - 3D vectors make it easy for humans to visualise, enabling us to plot and share insights about a corpus/specific document in graphical form\n",
    "* Classification problems - 3D vectors (or any low-dimensional vector space) are convenient for ML supervised learning (Classification) problems, as an algorithm can partition across the vector space with a plane/hyperplane, dividing up the space into classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most documents in a corpus will use many more distinct words, but such a topic model in this case will only be influenced by the use of such six words.\n",
    "* Extension - We can extend such approach to as many words as one specified (taking into account dimensionality) amongst an algorithm\n",
    "* Constraints - If the model only needed to separate documents according to three different dimensions/topics (given the thought experiments), the vocabulary could keep growing as desired\n",
    "\n",
    "In our thought experiments, we compressed six dimensions (TF-IDF normalized frequencies) into three dimensions (topics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat to this labour-intensive approach to semantic analysis is that it relies on human intuition and common sense to break down documents into topics, where common sense is hard to code into an algorithm.\n",
    "* Human constraint - A human couldn't allocate a sufficient amount of words to enough topics to precisely capture the meaning in any diverse corpus of documents we might want our machine to deal with\n",
    "* Algorithm - To automate this manual process, use an algorithm that doesn't rely on common sense to select topic weights for us\n",
    "* Computation - Each of these weighted sum is just a dot product. Three dot products (weighted sums) is just a matrix multiplication, or inner product. You multiply a 3 x n weight matrix with a TF-IDF vector (one value for each word in a document), where n is the number of terms in your vocabulary. The output of this multiplication is a new 3 x 1 topic vector for that document.\n",
    "* Vocabulary notation - The size of the vocabulary (the set of all possible words in a language) is usually written as $|V|$. And the variable $V$ alone is used to represent the set of possible words in your vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An algorithm for scoring topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an algorithmic way to determine these topic vectors - given a transformation from TF-IDF word vector representations into topic vectors.\n",
    "<br>\n",
    "A concept brought about by British linguist J.R Firth around the 20th century studied ways in which one can estimate what a word or morphene (smallest meaningful parts of a words such as in, come, -ing for incoming) signifies.\n",
    "<br>\n",
    "An excerpt from him in 1957 when referring on how to compute topics for words:\n",
    "> You should know a word by the company it keeps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the 'company' of a word, the most basic approach is to count co-occurrences in the same document.\n",
    "<br>\n",
    "Such word vector representations that were already covered via bag-of-words (BOW) and TF-IDF vectors.\n",
    "<br>\n",
    "This 'counting co-occurrences' approach led to the development of several algorithms for creating vectors to represent the statistics of word usage within documents or sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA thus comprises of an algorithm to analyse our TF-IDF matrix (table of TF-IDF vectors) to collate words into topics. It can work on BOW vectors as well, but TF-IDF vectors give slightly better results.\n",
    "* Process - LSA optimises such topics to maintain diversity in the topic dimensions. When we use these newly constructed topics instead of the original words, you still capture much of the meaning (semantics) of the documents\n",
    "* Dimensionality reduction - The number of topics one would need for a model to capture the meaning of your documents is far less than the number of words in the vocabulary of a standard TF-IDF vectors. LSA is often referred to as a dimension reduction technique. Hence, LSA reduces the number of dimensions one would need to capture the meaning of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other highly demanding ML tasks (related to dimensions for a large matrix of numbers) such a image analysis/classification, a common technique during the intermediary steps of the transformation is known as **Pricipal Components Analysis (PCA)**\n",
    "* PCA relevance - PCA uses the same computational processing or math as LSA\n",
    "* PCA discovery - Researchers only discovered recently that one could use PCA for semantic analysis of words, where it was only commonly found for reducing the dimensionality/tables of numbers, rather than TF-IDF/BOW vectors. Hence, they gave such application to NLP it own name as **LSA**\n",
    "* LSA synonyms - Under the field of information retrieval, where the focus is on creating indexes for full text search, LSA can also sometimes be referred to as **latent semantic indexing (LSI)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define further the concept of **indexing**, this is what databases do to be able to retrieve a certain row in a table quickly based on some partial information one would provide about such specific row.\n",
    "* Textbook index - When looking for a certain page, one could look up words in the index that should be on that page. Then it's easy enough to go straight to the page(s) that contain all the words that one is looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two algorithms are known to be similar to LSA, given similar NLP applications: \n",
    "<br>\n",
    "<br>\n",
    "1. **Linear discriminant analysis (LDA)** \n",
    "<br>\n",
    "2. **Latent Dirichlet allocation (LDiA)**\n",
    "<br>\n",
    "<br>\n",
    "LDA breaks down a document into only one topic. LDiA is more like LSA because it can break down documents into as many topics as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further technical note is that LDA doesn’t require **singular value decomposition (SVD)**. \n",
    "* We can just compute the centroid (average or mean) of all your TF-IDF vectors for each side of a binary class, like spam and nonspam\n",
    "* Our dimension then becomes the line between those two centroids \n",
    "* The further a TF-IDF vector is along that line (the dot product of the TF-IDF vector with that line) tells you how close you are to one class or another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An LDA classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with an example run of an LDA classifier to inform us about topic analysis will give us context to later methods through LSA and LDiA.\n",
    "* LDA level - LDA can be seen as one of the straightforward classification models and fast dimension reduction techniques\n",
    "* Applications - LDA for many applications can also have much better accuracy than more advanced algorithms recently devised\n",
    "* Functionality - Given it's a supervised algorithm, we need labels for our document class, but LDA requires relatively less samples than more complex algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified implementations of LDA, where the model 'training' contains only three steps:\n",
    "<br>\n",
    "1. Compute the average position (centroid) of all the TF-IDF vectors within the class (e.g. spam SMS messages)\n",
    "<br>\n",
    "2. Compute the average position of all the TF-IDF vectors within the class (e.g. nonspam SMS messages)\n",
    "<br>\n",
    "3. Compute the vector difference between the centroids (the line that connects them given a 3D depiction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of an LDA model is to find the vector (line) between the two centroids for our binary class (in this case 'spam' or 'ham')\n",
    "* Generating predictions - To generate inferences/predictions with such model, we need to find out if a new TF-IDF vector is closer to the in-class (spam) centroid than it is to the out-of-class (nonspam) centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train LDA model to classify SMS messages as spam or ham (nonspam)\n",
    "import pandas as pd \n",
    "from nlpia.data.loaders import get_data\n",
    "# Helps display the wide column of SMS text with a DataFrame printout\n",
    "pd.options.display.width = 120 \n",
    "sms = get_data(name='sms-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is optional but it makes it easier from a reporting perspective\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4837, 2)\n638\n       spam                                               text\nsms0      0  Go until jurong point, crazy.. Available only ...\nsms1      0                      Ok lar... Joking wif u oni...\nsms2!     1  Free entry in 2 a wkly comp to win FA Cup fina...\nsms3      0  U dun say so early hor... U c already then say...\nsms4      0  Nah I don't think he goes to usf, he lives aro...\nsms5!     1  FreeMsg Hey there darling it's been 3 week's n...\n"
    }
   ],
   "source": [
    "print(sms.shape) # rows, columns \n",
    "print(sms.spam.sum())\n",
    "print(sms.head(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this basic EDA, we have 4837 text messages and 638 of them are labeled with the binary class label 'spam'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to to a tokenization + TF-IDF vector transformation on all these SMS messages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(4837, 9232)\n638\n"
    }
   ],
   "source": [
    "# Inevitable situation where using a tfidf model increases dimensions (columns)\n",
    "print(tfidf_docs.shape)\n",
    "print(sms.spam.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nltk.casual_tokenizer` retrieves 9232 words in our vocabulary - illustrating almost twice as many words in our vocabulary and almost 10 times as many words as spam messages. Generally, the model wouldn't have a significant amount of information about the words that'll indicate whether a message is spam or not.\n",
    "<br>\n",
    "Also, a Naive Bayes classifer wouldn't operate as well when the vocabulary is much larger than the number of labeled examples in our dataset.\n",
    "<br>\n",
    "Hence, semantic analysis techniques covered here can help us deal with this constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.discriminant_analysis.LinearDiscriminant-Analysis can also be used \n",
    "# although it's convenient to just compute the centroids of our binary class (spam/ham)\n",
    "# to train this model \n",
    "mask = sms.spam.astype(bool).values # use values attribute to turn output contents into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.06 0.   0.   ... 0.   0.   0.  ]\n[0.02 0.01 0.   ... 0.   0.   0.  ]\n"
    }
   ],
   "source": [
    "# axis being '0' means the vectorization/functions are performed columnwise (independently)\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)\n",
    "print(spam_centroid.round(2))\n",
    "print(ham_centroid.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we take the difference between the spam and ham centroids to get the line between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-0.01, -0.02,  0.04, ..., -0.01, -0.  ,  0.  ])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# The dot product (each word vector * difference between spam and ham centroids) computes the projection of each vector on the line between the centroids \n",
    "spamminess_scores = tfidf_docs.dot(spam_centroid - ham_centroid)\n",
    "spamminess_scores.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spamminess_scores` is the distance along the line from the ham centroid to the spam centroid.\n",
    "<br>\n",
    "4837 dot products were computed simultaneously in a 'vectorised' numpy operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/3d-vector-scatterplot.png\" alt=\"3D scatter plot (point cloud) of our TF-IDF vectors\" width=\"400\" height='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 NLPIA Lane, Howard and Hapke (2019) chp 4.1.5 pp. 299 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the 3D scatter plot illustrating a view of TF-IDF vectors concerning its centroids from the SMS dataset (Figure 2), the arrow from the nonspam centroid to the spam centroid is the line that defines our trained model.\n",
    "<br>\n",
    "We can see how some of the green dots are on the back side of the arrow, so we can get the back side of the arrow - which can depict a ***negative*** spamminess score when we project them onto this line between the centroids. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be more easier to standardise the `spamminess_scores` value i.e. a score between 0 and 1 similar to a probablity.\n",
    "<br>\n",
    "`MinMaxScaler` from `sklearn` can helps us with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# as the API numpy recommends given these scores are a single column/feature \n",
    "# we 'drop' dimensions similar to transposing to get the correct column representation \n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_scores.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       spam  lda_predict  lda_score\nsms0      0            0       0.23\nsms1      0            0       0.18\nsms2!     1            1       0.72\nsms3      0            0       0.18\nsms4      0            0       0.29\nsms5!     1            1       0.55",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>spam</th>\n      <th>lda_predict</th>\n      <th>lda_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sms0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.23</td>\n    </tr>\n    <tr>\n      <th>sms1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>sms2!</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.72</td>\n    </tr>\n    <tr>\n      <th>sms3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>sms4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.29</td>\n    </tr>\n    <tr>\n      <th>sms5!</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.55</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "sms['lda_predict'] = (sms.lda_score > 0.5).astype(int)\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the first six messages were classified correctly given the threshold set at 50%. \n",
    "<br>\n",
    "Take the accuracy score to evaluate how the model performed on the rest of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.977"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# Using sklearn accuracy score function instead of filtering/computation version \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(sms.spam, sms.lda_predict).round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97.7% (3sf) of messages were correctly classified with this pretty simple LDA model.\n",
    "<br>\n",
    "Realistically, this isn't always the case when applied to real world data, as we haven't separated out a test set - for brevity given that we're generalising on 'seen' data only.\n",
    "<br>\n",
    "LDA is simple enough given it requires few parameters, where it should generalise well (that is on 'unseen data') so long as the SMS messages are already represented as the ones we intend to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ML model evaluation notes:***\n",
    "<br>\n",
    "The efficacy of semantic analysis approaches is that it doesn't necessarily rely on individual tokens unlike logistic or Naive Bayes models.\n",
    "* Semantics approach - Semantic analysis gathers up words with similar semantics (such as spamminess) and clusters them together to come up with a prediction\n",
    "* Considerations - This training set shown has a limited vocabulary and some non-English words in it. We would need to make sure our test messages use similar words if you want we want them to be classified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn # heatmap\n",
    "import matplotlib.pyplot as plt # visualisations output\n",
    "%matplotlib inline\n",
    "from pugnlp.stats import Confusion # convenient api that provides output similar to crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "lda_predict     0    1\nspam                  \n0            4135   64\n1              45  593",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>lda_predict</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4135</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>45</td>\n      <td>593</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "confusion_matrix = Confusion(sms['spam lda_predict'.split()])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"262.474375pt\" version=\"1.1\" viewBox=\"0 0 372.65825 262.474375\" width=\"372.65825pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.474375 \nL 372.65825 262.474375 \nL 372.65825 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 37.55625 224.64 \nL 305.39625 224.64 \nL 305.39625 7.2 \nL 37.55625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"QuadMesh_1\">\n    <path clip-path=\"url(#pd0beab54ed)\" d=\"M 37.55625 7.2 \nL 171.47625 7.2 \nL 171.47625 115.92 \nL 37.55625 115.92 \nL 37.55625 7.2 \n\" style=\"fill:#faebdd;\"/>\n    <path clip-path=\"url(#pd0beab54ed)\" d=\"M 171.47625 7.2 \nL 305.39625 7.2 \nL 305.39625 115.92 \nL 171.47625 115.92 \nL 171.47625 7.2 \n\" style=\"fill:#04051a;\"/>\n    <path clip-path=\"url(#pd0beab54ed)\" d=\"M 37.55625 115.92 \nL 171.47625 115.92 \nL 171.47625 224.64 \nL 37.55625 224.64 \nL 37.55625 115.92 \n\" style=\"fill:#03051a;\"/>\n    <path clip-path=\"url(#pd0beab54ed)\" d=\"M 171.47625 115.92 \nL 305.39625 115.92 \nL 305.39625 224.64 \nL 171.47625 224.64 \nL 171.47625 115.92 \n\" style=\"fill:#33183c;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7da8ca2f28\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"104.51625\" xlink:href=\"#m7da8ca2f28\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(101.335 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.43625\" xlink:href=\"#m7da8ca2f28\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(235.255 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- lda_predict -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 50.984375 -16.609375 \nL 50.984375 -23.578125 \nL -0.984375 -23.578125 \nL -0.984375 -16.609375 \nz\n\" id=\"DejaVuSans-95\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n     </defs>\n     <g transform=\"translate(143.771563 252.916562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"91.259766\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-95\"/>\n      <use x=\"202.539062\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"266.015625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"307.097656\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"368.621094\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"432.097656\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"459.880859\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"514.861328\" xlink:href=\"#DejaVuSans-116\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m388bee9287\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m388bee9287\" y=\"61.56\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(28.476563 64.123281)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.55625\" xlink:href=\"#m388bee9287\" y=\"170.28\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1 -->\n      <g transform=\"translate(28.476563 172.843281)rotate(-90)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- spam -->\n     <defs>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     </defs>\n     <g transform=\"translate(14.798438 129.633281)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"52.099609\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"115.576172\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"176.855469\" xlink:href=\"#DejaVuSans-109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"text_7\">\n    <!-- 4135 -->\n    <defs>\n     <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n     <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n     <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(91.79125 64.319375)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-49\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-51\"/>\n     <use x=\"190.869141\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n   <g id=\"text_8\">\n    <!-- 64 -->\n    <defs>\n     <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(232.07375 64.319375)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-54\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\n    </g>\n   </g>\n   <g id=\"text_9\">\n    <!-- 45 -->\n    <g style=\"fill:#ffffff;\" transform=\"translate(98.15375 173.039375)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-52\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n    </g>\n   </g>\n   <g id=\"text_10\">\n    <!-- 593 -->\n    <defs>\n     <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n    </defs>\n    <g style=\"fill:#ffffff;\" transform=\"translate(228.8925 173.039375)scale(0.1 -0.1)\">\n     <use xlink:href=\"#DejaVuSans-53\"/>\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-57\"/>\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-51\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p0f108b7568)\" d=\"M 322.13625 224.64 \nL 322.13625 223.790625 \nL 322.13625 8.049375 \nL 322.13625 7.2 \nL 333.00825 7.2 \nL 333.00825 8.049375 \nL 333.00825 223.790625 \nL 333.00825 224.64 \nz\n\" style=\"fill:#ffffff;stroke:#ffffff;stroke-linejoin:miter;stroke-width:0.01;\"/>\n   </g>\n   <image height=\"217\" id=\"image2be746d0e5\" transform=\"scale(1 -1)translate(0 -217)\" width=\"11\" x=\"322\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAsAAADZCAYAAAD2WsoCAAAABHNCSVQICAgIfAhkiAAAAVNJREFUaIHVmskNw0AMxHS5ifRfZ5wWxAcBxe/BYERpDxvOeT5vLJ/JzK2WigOIu0pyLq3AIjGQM6LhxfA4d5J2i005MXWMhjf8N5wRjSvDf6TAtdSdDY0GjLF/zE2GiEebZ0bDa7fm7A2/hw7SADFEdDcK9DJ7MdjqJs6swPXrwd/SIGJCAxZ4IgbcN0gMkvkvaXiz8e6tzY1Ri9FBnBlnKzPkDGIwGkgM+i2iS7AXTBNxETGKAZ3X2piur+QsFggWIctcDWjAzCAGQodoVJ/gzJw1dDVWjPBmwxODe8+EFgM6k5s5uHGwGIFiELEYAxyb0Jl8skDOUEy+12k0gnw29GjkNHAWM5/gzGhcaTdxRmsQvPKqNEgHUYxGBWqcmbOFLhoNvxbDKxA5e7NxhAYq0FuweWPqRBreCYs2xkdyvrJgEWfSbvSXxZUC0bVHPAelAn9HkCsjRlNkkwAAAABJRU5ErkJggg==\" y=\"-7\"/>\n   <g id=\"matplotlib.axis_3\"/>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_3\">\n     <g id=\"line2d_5\">\n      <defs>\n       <path d=\"M 0 0 \nL 3.5 0 \n\" id=\"m91c07f3d45\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"200.450465\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 500 -->\n      <g transform=\"translate(340.00825 204.249683)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"173.868557\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1000 -->\n      <g transform=\"translate(340.00825 177.667776)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"147.28665\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1500 -->\n      <g transform=\"translate(340.00825 151.085869)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"120.704743\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 2000 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(340.00825 124.503962)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"94.122836\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2500 -->\n      <g transform=\"translate(340.00825 97.922055)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"67.540929\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 3000 -->\n      <g transform=\"translate(340.00825 71.340148)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"40.959022\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 3500 -->\n      <g transform=\"translate(340.00825 44.758241)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.00825\" xlink:href=\"#m91c07f3d45\" y=\"14.377115\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 4000 -->\n      <g transform=\"translate(340.00825 18.176334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 322.13625 224.64 \nL 322.13625 223.790625 \nL 322.13625 8.049375 \nL 322.13625 7.2 \nL 333.00825 7.2 \nL 333.00825 8.049375 \nL 333.00825 223.790625 \nL 333.00825 224.64 \nz\n\" style=\"fill:none;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd0beab54ed\">\n   <rect height=\"217.44\" width=\"267.84\" x=\"37.55625\" y=\"7.2\"/>\n  </clipPath>\n  <clipPath id=\"p0f108b7568\">\n   <rect height=\"217.44\" width=\"10.872\" x=\"322.13625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# seaborn heatmap visual to get a better idea of the algorithm performance\n",
    "# detecting false positives, true negatives etc\n",
    "sn.heatmap(data=confusion_matrix, annot=True, fmt='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is convenient for further making granular the accuracy of such model in terms of identifying (in this case) SMS messages labelled as spam when it wasn't spam at all (**false positives**) and messages labelled as ham that instead should be labelled as spam (**false negatives**)\n",
    "* Adjustment - Adjusting the 0.5 threshold can somewhat change the distribution of false positives (currently 64 in top right hand square) and false negatives (currently 45 in the lower left hand square), perhaps to anchor more to desired balance\n",
    "* Limitations - Thus far under LDA, the main thing the 1D vectors 'understand' is the spamminess of words and documents. We'd also like to learn more regarding word nuances and give us a multidimensional vector that captures a word’s meaning."
   ]
  }
 ]
}