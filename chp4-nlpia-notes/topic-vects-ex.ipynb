{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding meaning in words counts (semantic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic analysis** emphasises the use of machines/programmes to understand the 'meaning' of words.\n",
    "<br>\n",
    "A reminder of TF-IDF word vectors scores for n-grams is that the application is useful for searching text if the exact words/n-grams that are to be subsequently search are known. \n",
    "<br>\n",
    "NLP applications in the past have found such algorithms for revealing the meaning of word combinations and computing vectors to represent this meaning - ***latent semantic analysis (LSA)***. Utilising this tool not only represents the meaning of words as vectors, but can also be used to represent the meaning of entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning more about ***semantic/topic vectors***, we can use weighted frequency scores from TFIDF vectors to compute the topic 'scores' that make up the dimensions of our topic vectors. The idea is to use the correlation of normalized term frequencies with each other to group words together in topics to define the dimensions of our new topic vectors.\n",
    "<br>\n",
    "Such methods make it possible to utilise interesting applications e.g. making it possible to search for documents based on their meaning - ***semantic search***. At times, semantic search returns search results that are much better than keyword search (TF-IDF search). It can return documents that are exactly what the user is searching for, even when the user can't think of the right words to put in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic vectors help us identify the words or n-grams that best represent the subject (topic) of a statement, document or corpus (collection of documents). Given this vector representation of words along with their *relative* importance, you can provide someone with the most meaningful words for a document - a set of keywords that summarizes its meaning.\n",
    "<br>\n",
    "Semantic vectors enable the possibility to compare any two statements/documents and tell how 'close' they are in *meaning* to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations (linear) of words that make up the dimensions of our topic vectors are powerful representations of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts to topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to score the meanings and topics the words are used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectors and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any word vector representation such as TF-IDF count the exact spellings of terms in a document. \n",
    "<br>\n",
    "As a reminder, texts that restate the same meaning will have completely different TF-IDF vector representations if they spell things differently or use different words. Such cases can confuse search engines and also document similarity comparisons relying on token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization approach kept similarly *spelled* words together within an analysis, but not necessarily words with similar meanings - failing to pair up most synonyms.\n",
    "<br>\n",
    "This is challenging as synonyms differ in more ways than just word endings that lemmatization and stemming deal with.\n",
    "<br>\n",
    "Sometimes, lemmatization and stemming can actually mistakenly group together antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, two chunks of text that talk about the same thing but use different words will not be 'close' to each other in our lemmatized TF-IDF vector space model.\n",
    "<br> \n",
    "An instance might be that the TF-IDF vector for one chapter in a book about NLP may not be close at all to similar-meaning passages in university textbooks about latent semantic indexing.\n",
    "<br>\n",
    "Generally, the NLP book might use more modernised jargon/terms than the university textbook, where university researchers use more consistent and rigorous language within textbooks/lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to search for a more optimal method to extract additional information and meaning from word statistics i.e. a better estimate of what the words in a document 'signify'. \n",
    "<br>\n",
    "Also, being wary of a better estimate of what the words in a document 'signify', all the while trying to understand what the combination of words *means* in a certain document. This would mean we'd like to represent that meaning with a vector that's like a TF-IDF vector, yet more compact and meanigful.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come up with two terms to describe the context:\n",
    "<br>\n",
    "1. **word-topic vectors** - Compact meaning vectors \n",
    "<br>\n",
    "2. **document-topic** - Document meaning vectors \n",
    "<br>\n",
    "<br>\n",
    "In any case, either of these can be called **topic vectors**\n",
    "<br>\n",
    "Topic vectors can be compact or as expansive (high dimensions) as desired. LSA topic vectors can have as little as one dimension or thousands of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mathematical operations between topic vectors (addition and subtraction) mean more than they did with TF-IDF vectors.\n",
    "<br>\n",
    "The distances between topic vectors is useful for things like clustering documents or semantic search (including search by semantics) - whereas doing TF-IDF topic modelling could only cluster and search using keywords via TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these computations are completed, we'll have one document-topic vector for each document in the corpus - which also usually translates to not having to reprocess the entire corpus to compute a new topic vector for a new document or phrase.\n",
    "<br>\n",
    "We'll have a topic vector for each word in our lexicon (vocabulary), where we can use/compute these word topic vectors for any new document by adding up all its word topic vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical representations of the semantics of words/sentences can be tricky.\n",
    "<br>\n",
    "Given languages like English have multiple diaclects and different interpretations of the same words - the concept of words with multiple meanings is known as ***polysemy***:\n",
    "* ***polysemy*** - The existence of words and phrases with more than one meaning \n",
    "\n",
    "Some ways polysemy can affect the semantics of word/statements. LSA actually manages handling these situations for us:\n",
    "* ***Homonyms*** - Words with the same spelling and pronounciation but different meanings e.g Bat (baseball bat and animal)\n",
    "* ***Zeugma*** - Use of two meanings of a word simultaneously in the same sentence e.g. I held **her hand** and **my tongue**\n",
    "\n",
    "LSA also deals with some of the challenges of polysemy in a voice interface (chatbot) that one can talk to, like Alexa or Siri:\n",
    "* ***Homographs*** — Words spelled the same, but with different pronunciations and meanings e.g. Bass (type of fish OR low deep voice)\n",
    "* ***Homophones*** - Words with the same pronunciation, but different spellings and meanings (an NLP challenge with voice interfaces) e.g. Blew and Blue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have a TF-IDF vector for a certain document and we want to convert that to a topic vector - think about how much each contributes to such named topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s say we're processing some sentences about pets in Central Park in New York City (NYC). We can create three topics: one about pets, one about animals, and another about cities. \n",
    "<br>\n",
    "Call these topics 'petness', 'animalness', and 'cityness'. So your 'petness' topic about pets will score words like 'cat' and 'dog' significantly, but probably ignore words like 'NYC' and 'apple'. The 'cityness' topic will ignore words like 'cat' and 'dog', but might give a little weight to 'apple', just because of the 'Big Apple' association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example would be where we 'train' the topic model as specified, and without using a computerised based solution (solely logic/common sense), we might come up with some weights like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tf-idf dict with randomised scores/weights\n",
    "topic = {} \n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['petness'] = (0.3 * tfidf['cat'] +\\\n",
    "                    0.3 * tfidf['dog'] +\\\n",
    "                    0 * tfidf['apple'] + 0 * tfidf['lion'] - 0.2 * tfidf['NYC'] +\\\n",
    "                    0.2 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['animalness'] = (0.1 * tfidf['cat'] +\\\n",
    "                       0.1 * tfidf['dog'] +\\\n",
    "                       0.1 * tfidf['apple'] + 0.5 * tfidf['lion'] + 0.1 * tfidf['NYC'] -\\\n",
    "                       0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['cityness'] = (0 * tfidf['cat'] -\\\n",
    "                     0.1 * tfidf['dog'] +\\\n",
    "                     0.2 * tfidf['apple'] - 0.1 * tfidf['lion'] + 0.5 * tfidf['NYC'] +\\\n",
    "                     0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual weights e.g like in 'petness' topic (0.3, 0.3, 0, 0, -0.2, 0.2) are multiplied by the randomised/imaginary tfidf values to create topic vectors for this imaginary/random document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we added up word frequencies that might be indicators of each topics.\n",
    "<br>\n",
    "We weighted the word frequencies (TF-IDF values) by how likely the word is associated with the topic. Similarly, for words that might be talking about something that is in some way the opposite of our topic, -ve weights would mean a disimilarity.\n",
    "<br>\n",
    "Running through this process, we get an illustration and simulation of how we teach a machine to think closer towards a human in term of natural language.\n",
    "<br>\n",
    "We arbitrarily chose to decompose our words and documents into only three topics ('petness', 'animalness' and 'cityness') - with a limited vocabulary of only six words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next high level step is to think how a human might decide mathematically which topics/words are connected, along with what specific weights such connections should have.\n",
    "* positve associations within topic - The terms 'cat' and 'dog' should have similar contributions to the 'petness' topic (weight of 0.3)\n",
    "* Negative associations within topic - The term 'NYC' should have a negative weight for the 'petness' topic. In general, city names, proper names, abbreviations and acronyms share little in common with words about pets.\n",
    "\n",
    "N.B - It's a good idea to keep a note of what 'sharing in common' means for words, ideally searching for something in a TF-IDF matrix that represents the meaning the words share in common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***topic vectors weight computation:***\n",
    "<br>\n",
    "<br>\n",
    "Given the above signed weighting of words to produce topic vectors, given it was done manually, we decided to normalize the topic vectors via the $L^2$-norm.\n",
    "<br>\n",
    "Genuine applications of LSA normalizes topic vectors in a similar way as well via the $L^2$-norm. \n",
    "<br>\n",
    "$L^2$-norm is simply the traditional Euclidean distance/length that is conventionally seen in basic geometry. \n",
    "<br>\n",
    "It can be seen as the Pythagorean theorem solved for the length of the hypotenuse of a right triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships between words and topics can be reversed.\n",
    "<br>\n",
    "The 3 x 6 matrix of three topics can be transposed to produce topic weights for each word in our vocabulary (i.e. 6 x 3 matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These vectors of weights would be our word vectors for our 6 words \n",
    "word_vector = {}\n",
    "word_vector['cat'] = 0.3 * topic['petness'] +\\\n",
    "                     0.1 * topic['animalness'] +\\\n",
    "                     0 * topic['cityness']\n",
    "word_vector['dog'] = 0.3 * topic['petness'] +\\\n",
    "                     0.1 * topic['animalness'] -\\\n",
    "                     0.1 * topic['cityness']\n",
    "word_vector['apple'] =  0 * topic['petness'] -\\\n",
    "                        0.1 * topic['animalness'] +\\\n",
    "                        0.2 * topic['cityness']  \n",
    "word_vector['lion'] = 0 * topic['petness'] +\\\n",
    "                      0.5 * topic['animalness'] -\\\n",
    "                      0.1 * topic['cityness']  \n",
    "word_vector['NYC'] =  -0.2 * topic['petness'] +\\\n",
    "                      0.1 * topic['animalness'] +\\\n",
    "                      0.5 * topic['cityness']\n",
    "word_vector['love'] = 0.2 * topic['petness'] -\\\n",
    "                      0.1 * topic['animalness'] +\\\n",
    "                      0.1 * topic['cityness']                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pathlib import Path\n",
    "#import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = Path().home()/'Desktop'/'nlp-map-project'/'chp4-nlpia-notes'/'img-vects'\n",
    "#os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"img-vects/3d-vector-depiction-topic-vectors.png\" alt=\"3D vectors referring to six words and associated topic vectors\" width=\"400\" height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 NLPIA Lane, Howard and Hapke (2019) chp 4.1.3 pp. 287 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in Figure 1, these six topic vectors (one for each word) represent the meanings of our six words as 3D vectors.\n",
    "* Counting up occurrences of these six words and multiplying them by our weights retrieves the 3D topic vector for any document\n",
    "* Illustration - 3D vectors make it easy for humans to visualise, enabling us to plot and share insights about a corpus/specific document in graphical form\n",
    "* Classification problems - 3D vectors (or any low-dimensional vector space) are convenient for ML supervised learning (Classification) problems, as an algorithm can partition across the vector space with a plane/hyperplane, dividing up the space into classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most documents in a corpus will use many more distinct words, but such a topic model in this case will only be influenced by the use of such six words.\n",
    "* Extension - We can extend such approach to as many words as one specified (taking into account dimensionality) amongst an algorithm\n",
    "* Constraints - If the model only needed to separate documents according to three different dimensions/topics (given the thought experiments), the vocabulary could keep growing as desired\n",
    "\n",
    "In our thought experiments, we compressed six dimensions (TF-IDF normalized frequencies) into three dimensions (topics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat to this labour-intensive approach to semantic analysis is that it relies on human intuition and common sense to break down documents into topics, where common sense is hard to code into an algorithm.\n",
    "* Human constraint - A human couldn't allocate a sufficient amount of words to enough topics to precisely capture the meaning in any diverse corpus of documents we might want our machine to deal with\n",
    "* Algorithm - To automate this manual process, use an algorithm that doesn't rely on common sense to select topic weights for us\n",
    "* Computation - Each of these weighted sum is just a dot product. Three dot products (weighted sums) is just a matrix multiplication, or inner product. You multiply a 3 x n weight matrix with a TF-IDF vector (one value for each word in a document), where n is the number of terms in your vocabulary. The output of this multiplication is a new 3 x 1 topic vector for that document.\n",
    "* Vocabulary notation - The size of the vocabulary (the set of all possible words in a language) is usually written as $|V|$. And the variable $V$ alone is used to represent the set of possible words in your vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An algorithm for scoring topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an algorithmic way to determine these topic vectors - given a transformation from TF-IDF word vector representations into topic vectors.\n",
    "<br>\n",
    "A concept brought about by British linguist J.R Firth around the 20th century studied ways in which one can estimate what a word or morphene (smallest meaningful parts of a words such as in, come, -ing for incoming) signifies.\n",
    "<br>\n",
    "An excerpt from him in 1957 when referring on how to compute topics for words:\n",
    "> You should know a word by the company it keeps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To infer the 'company' of a word, the most basic approach is to count co-occurrences in the same document.\n",
    "<br>\n",
    "Such word vector representations that were already covered via bag-of-words (BOW) and TF-IDF vectors.\n",
    "<br>\n",
    "This 'counting co-occurrences' approach led to the development of several algorithms for creating vectors to represent the statistics of word usage within documents or sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA thus comprises of an algorithm to analyse our TF-IDF matrix (table of TF-IDF vectors) to collate words into topics. It can work on BOW vectors as well, but TF-IDF vectors give slightly better results.\n",
    "* Process - LSA optimises such topics to maintain diversity in the topic dimensions. When we use these newly constructed topics instead of the original words, you still capture much of the meaning (semantics) of the documents\n",
    "* Dimensionality reduction - The number of topics one would need for a model to capture the meaning of your documents is far less than the number of words in the vocabulary of a standard TF-IDF vectors. LSA is often referred to as a dimension reduction technique. Hence, LSA reduces the number of dimensions one would need to capture the meaning of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other highly demanding ML tasks (related to dimensions for a large matrix of numbers) such a image analysis/classification, a common technique during the intermediary steps of the transformation is known as **Pricipal Components Analysis (PCA)**\n",
    "* PCA relevance - PCA uses the same computational processing or math as LSA\n",
    "* PCA discovery - Researchers only discovered recently that one could use PCA for semantic analysis of words, where it was only commonly found for reducing the dimensionality/tables of numbers, rather than TF-IDF/BOW vectors. Hence, they gave such application to NLP it own name as **LSA**\n",
    "* LSA synonyms - Under the field of information retrieval, where the focus is on creating indexes for full text search, LSA can also sometimes be referred to as **latent semantic indexing (LSI)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define further the concept of **indexing**, this is what databases do to be able to retrieve a certain row in a table quickly based on some partial information one would provide about such specific row.\n",
    "* Textbook index - When looking for a certain page, one could look up words in the index that should be on that page. Then it's easy enough to go straight to the page(s) that contain all the words that one is looking for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two algorithms are known to be similar to LSA, given similar NLP applications: \n",
    "<br>\n",
    "<br>\n",
    "1. **Linear discriminant analysis (LDA)** \n",
    "<br>\n",
    "2. **Latent Dirichlet allocation (LDiA)**\n",
    "<br>\n",
    "<br>\n",
    "LDA breaks down a document into only one topic. LDiA is more like LSA because it can break down documents into as many topics as you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further technical note is that LDA doesn’t require **singular value decomposition (SVD)**. \n",
    "* We can just compute the centroid (average or mean) of all your TF-IDF vectors for each side of a binary class, like spam and nonspam\n",
    "* Our dimension then becomes the line between those two centroids \n",
    "* The further a TF-IDF vector is along that line (the dot product of the TF-IDF vector with that line) tells you how close you are to one class or another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}