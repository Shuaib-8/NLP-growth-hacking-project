{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding meaning in words counts (semantic analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic analysis** emphasises the use of machines/programmes to understand the 'meaning' of words.\n",
    "<br>\n",
    "A reminder of TF-IDF word vectors scores for n-grams is that the application is useful for searching text if the exact words/n-grams that are to be subsequently search are known. \n",
    "<br>\n",
    "NLP applications in the past have found such algorithms for revealing the meaning of word combinations and computing vectors to represent this meaning - ***latent semantic analysis (LSA)***. Utilising this tool not only represents the meaning of words as vectors, but can also be used to represent the meaning of entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning more about ***semantic/topic vectors***, we can use weighted frequency scores from TFIDF vectors to compute the topic 'scores' that make up the dimensions of our topic vectors. The idea is to use the correlation of normalized term frequencies with each other to group words together in topics to define the dimensions of our new topic vectors.\n",
    "<br>\n",
    "Such methods make it possible to utilise interesting applications e.g. making it possible to search for documents based on their meaning - ***semantic search***. At times, semantic search returns search results that are much better than keyword search (TF-IDF search). It can return documents that are exactly what the user is searching for, even when the user can't think of the right words to put in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic vectors help us identify the words or n-grams that best represent the subject (topic) of a statement, document or corpus (collection of documents). Given this vector representation of words along with their *relative* importance, you can provide someone with the most meaningful words for a document - a set of keywords that summarizes its meaning.\n",
    "<br>\n",
    "Semantic vectors enable the possibility to compare any two statements/documents and tell how 'close' they are in *meaning* to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinations (linear) of words that make up the dimensions of our topic vectors are powerful representations of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts to topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to score the meanings and topics the words are used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectors and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any word vector representation such as TF-IDF count the exact spellings of terms in a document. \n",
    "<br>\n",
    "As a reminder, texts that restate the same meaning will have completely different TF-IDF vector representations if they spell things differently or use different words. Such cases can confuse search engines and also document similarity comparisons relying on token counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatization approach kept similarly *spelled* words together within an analysis, but not necessarily words with similar meanings - failing to pair up most synonyms.\n",
    "<br>\n",
    "This is challenging as synonyms differ in more ways than just word endings that lemmatization and stemming deal with.\n",
    "<br>\n",
    "Sometimes, lemmatization and stemming can actually mistakenly group together antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequently, two chunks of text that talk about the same thing but use different words will not be 'close' to each other in our lemmatized TF-IDF vector space model.\n",
    "<br> \n",
    "An instance might be that the TF-IDF vector for one chapter in a book about NLP may not be close at all to similar-meaning passages in university textbooks about latent semantic indexing.\n",
    "<br>\n",
    "Generally, the NLP book might use more modernised jargon/terms than the university textbook, where university researchers use more consistent and rigorous language within textbooks/lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to search for a more optimal method to extract additional information and meaning from word statistics i.e. a better estimate of what the words in a document 'signify'. \n",
    "<br>\n",
    "Also, being wary of a better estimate of what the words in a document 'signify', all the while trying to understand what the combination of words *means* in a certain document. This would mean we'd like to represent that meaning with a vector that's like a TF-IDF vector, yet more compact and meanigful.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We come up with two terms to describe the context:\n",
    "<br>\n",
    "1. **word-topic vectors** - Compact meaning vectors \n",
    "<br>\n",
    "2. **document-topic** - Document meaning vectors \n",
    "<br>\n",
    "<br>\n",
    "In any case, either of these can be called **topic vectors**\n",
    "<br>\n",
    "Topic vectors can be compact or as expansive (high dimensions) as desired. LSA topic vectors can have as little as one dimension or thousands of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mathematical operations between topic vectors (addition and subtraction) mean more than they did with TF-IDF vectors.\n",
    "<br>\n",
    "The distances between topic vectors is useful for things like clustering documents or semantic search (including search by semantics) - whereas doing TF-IDF topic modelling could only cluster and search using keywords via TF-IDF vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these computations are completed, we'll have one document-topic vector for each document in the corpus - which also usually translates to not having to reprocess the entire corpus to compute a new topic vector for a new document or phrase.\n",
    "<br>\n",
    "We'll have a topic vector for each word in our lexicon (vocabulary), where we can use/compute these word topic vectors for any new document by adding up all its word topic vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical representations of the semantics of words/sentences can be tricky.\n",
    "<br>\n",
    "Given languages like English have multiple diaclects and different interpretations of the same words - the concept of words with multiple meanings is known as ***polysemy***:\n",
    "* ***polysemy*** - The existence of words and phrases with more than one meaning \n",
    "\n",
    "Some ways polysemy can affect the semantics of word/statements. LSA actually manages handling these situations for us:\n",
    "* ***Homonyms*** - Words with the same spelling and pronounciation but different meanings e.g Bat (baseball bat and animal)\n",
    "* ***Zeugma*** - Use of two meanings of a word simultaneously in the same sentence e.g. I held **her hand** and **my tongue**\n",
    "\n",
    "LSA also deals with some of the challenges of polysemy in a voice interface (chatbot) that one can talk to, like Alexa or Siri:\n",
    "* ***Homographs*** — Words spelled the same, but with different pronunciations and meanings e.g. Bass (type of fish OR low deep voice)\n",
    "* ***Homophones*** - Words with the same pronunciation, but different spellings and meanings (an NLP challenge with voice interfaces) e.g. Blew and Blue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thought experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we have a TF-IDF vector for a certain document and we want to convert that to a topic vector - think about how much each contributes to such named topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let’s say we're processing some sentences about pets in Central Park in New York City (NYC). We can create three topics: one about pets, one about animals, and another about cities. \n",
    "<br>\n",
    "Call these topics 'petness', 'animalness', and 'cityness'. So your 'petness' topic about pets will score words like 'cat' and 'dog' significantly, but probably ignore words like 'NYC' and 'apple'. The 'cityness' topic will ignore words like 'cat' and 'dog', but might give a little weight to 'apple', just because of the 'Big Apple' association."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example would be where we 'train' the topic model as specified, and without using a computerised based solution (solely logic/common sense), we might come up with some weights like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tf-idf dict with randomised scores/weights\n",
    "topic = {} \n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['petness'] = (0.3 * tfidf['cat'] +\\\n",
    "                    0.3 * tfidf['dog'] +\\\n",
    "                    0 * tfidf['apple'] + 0 * tfidf['lion'] - 0.2 * tfidf['NYC'] +\\\n",
    "                    0.2 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['animalness'] = (0.1 * tfidf['cat'] +\\\n",
    "                       0.1 * tfidf['dog'] +\\\n",
    "                       0.1 * tfidf['apple'] + 0.5 * tfidf['lion'] + 0.1 * tfidf['NYC'] -\\\n",
    "                       0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic['cityness'] = (0 * tfidf['cat'] -\\\n",
    "                     0.1 * tfidf['dog'] +\\\n",
    "                     0.2 * tfidf['apple'] - 0.1 * tfidf['lion'] + 0.5 * tfidf['NYC'] +\\\n",
    "                     0.1 * tfidf['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual weights e.g like in 'petness' topic (0.3, 0.3, 0, 0, -0.2, 0.2) are multiplied by the randomised/imaginary tfidf values to create topic vectors for this imaginary/random document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we added up word frequencies that might be indicators of each topics.\n",
    "<br>\n",
    "We weighted the word frequencies (TF-IDF values) by how likely the word is associated with the topic. Similarly, for words that might be talking about something that is in some way the opposite of our topic, -ve weights would mean a disimilarity.\n",
    "<br>\n",
    "Running through this process, we get an illustration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}