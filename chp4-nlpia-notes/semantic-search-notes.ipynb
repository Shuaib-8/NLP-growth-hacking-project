{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Topic vectors "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Topic vectors add more tools for assessing textual data (search queries), such as comparing the meaning of words, documents, statements and corpora.\n",
    "<br>\n",
    "This stems from retrieving 'clusters' of similar documents and statements. In this way, there's no longer a need to compare the distance (cosine similarity) between documents based merely on their word usage.\n",
    "<br>\n",
    "In this sense, users are no longer limited to keyword search and relevance ranking based entirely on word choice or vocabulary. We can find documents that are relevant to our query - not just a highlighted match for word statistics per se."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Hence, this paradigm is known as **semantic search**, which is what strong search engines are able to do when they they are given documents that do not contain many of the words in our query, but are exactly what one is/perhaps looking for. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sematic search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Semantic search gives us a tool for finding and generating meaningful text.\n",
    "<br>\n",
    "When we search for a document based on a word or partial phrase it contains, which is normally known as ***full text search***.\n",
    "<br>\n",
    "This is the typical purpose of a search engine - which breaks down a document into chunks (words) that can be indexed via a *inverted index* as usually found at the back of a non-fiction book. It can take a lot of auditing and guesswork to deal with spelling errors and typos - but it generally works well for simple referencing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***Semantic search*** is full text serch that takes into account the meaning of the words in a corresponding query and document one is searching for.\n",
    "<br>\n",
    "As with prior notebooks within this repo, there are two ways - LSA (PCA) and Latent Dirichlet allocation (LDiA) - to compute topic vectors that capture the semantics (meaning) of words and documents in a vector.\n",
    "<br>\n",
    "One of the reasons that latent semantic analysis (LSA) was initially known as latent semantic ***indexing*** (LSI) was because it was said to power semantic search with an index of numerical values, like BOW and TF-IDF tables. In this way, sematic search was then ext big thing in information retrieval."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "However, unlike BOW and TF-IDF tables, tables of semantic vectors cannot be easily discretized and index using traditional inverted index techniques.\n",
    "<br>\n",
    "<br>\n",
    "Traditional indexing approaches work with binary word occurrence vectors, discrete vectors (BOW vectors), sparse continuous vectors (TF-IDF vectors), and low-dimensional continuous, such as topic vectors from LSA/LDiA are a challenge.\n",
    "<br>\n",
    "Inverted indexes work for discrete vectors or binary vectors, like tables of binary/integer word-document vectors, because the index only needs to maintain an entry for each nonzero discrete dimension. Given that most of the vector transformations mentioned above such as TF-IDF vectors are sparse (mostly zero), we don't need an entry in our index for most dimensions for most documents."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "LSA/LDiA produce topic vectors that are high-dimensional, continous and dense (i.e. zero values are rare). Also, the semantic analysis algorithm doesn't produce an efficient for scalable search.\n",
    "<br>\n",
    "One solution to the challenge of high-dimensional vectors is to index them with a ***locality sensitive hash (LSH)***. LSH is like a postal code that designates a region of hyperspace so that it can easily be found again later. Also, like a regular hash, it's discrete and depends only on the values in the vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src=\"img-vects/semantic-search-accuracy.png\" alt=\"Semantic search accuracy deteriorates at around 12-D\" width=\"400\" height='250'/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Figure 1 NLPIA Lane, Howard and Hapke (2019) chp 4.8.1 pp. 372 Apple iBooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "As seen from figure 1, this process doesn't work perfectly once we exceed about 12 dimensions. \n",
    "Also in figure 1, from column one, each row represents a topic vector size (dimensionality), starting with 2 dimensions and working up to 16 dimensions "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The table shows how good one's search results would be if you LSH were used to index a large number of semantic vectors.\n",
    "A noticeable heuristic is that once a specified vector has more than 16 dimensions, we'd have a hard time returning two search results that were any good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> How can we do semantic search on 100-D vectors without an index?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To find precise semantic matches, we need to find all the closest document topic vectors to a particular query (search) topic vector.\n",
    "<br>\n",
    "If we have *n* documents, we need to do *n* comparisons with our query topic vector - which is a lost of dot products."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Also, we can vectorize the operation in numpy via matrix multiplication; yet this doesn't reduce to number (order) of operation i.e. it only makes the computations (run-time) 100 times faster.\n",
    "<br>\n",
    "Precise semantic search still requires O(N) - linear complexity - multiplications and additions for each query. Hence, it scales linearly with the size of our corpus (n as input). \n",
    "<br>\n",
    "Arguably, this wouldn't work for a large corpus, such as Google Search or even Wikipedia semantic search.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The key is to settle for 'good enough' rather than striving for a perfect index of LSH algorithm for our high-dimensional vectors.\n",
    "<br>\n",
    "There are for some time now several open source implementations of some efficient and accurate ***approximate nearest neighbors*** algorithms that use LSH to efficiently implement semantic search.\n",
    "<br>\n",
    "A couple of the easiest to use and install are:\n",
    "* `Spotify` - `Annoy` package \n",
    "* `Gensim` - `gensim.models.KeyedVector` class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Hence, these indexing or hashing solutions cannot gurantee that we'll find all the best matches for our semantic search query.\n",
    "<br>\n",
    "However, they can get us back a good list of close matches almost as fast as with a conventional reverse index on a TF-IDF/BOW vector, provided we're willing to give up a little precision."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}