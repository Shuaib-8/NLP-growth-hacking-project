{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDiA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Sematic Analysis (LSA) should be the first choice for most topic modelling, semantic search, or content-based recommendation engines.\n",
    "<br>\n",
    "Given the maths used for LSA is efficient and somewhat easy to follow, given that it produces a linear transformation that can be applied to new batches of natural language without training and little loss in accuracy. However, LDiA can give slightly better results in some situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDiA performs many things accomplished by topic models with LSA (and SVD under the hood), though unlike LSA, LDiA assumes a Dirichlet distribution of word frequencies. It's more precise about the statistics of allocating words to topics than the linear math of LSA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDiA creates a semantic vector space model (as with topic vectors) using a topic mix process that for any given document, such topics can be determined by the word mixtures in each topic by which topic those words were assigned to.\n",
    "<br>\n",
    "In some ways, this makes an LDiA topic model easier to understand, as words assigned to topics and topics assigned to documents tend to make more sense than for LSA.\n",
    "<br>\n",
    "LDiA assumes that each document is a mixture (linear combination) of some arbitrary number of topics that one can select when beginning to train the LDiA model. LDiA also assumes that each topic can be represented by a distribution of words (term frequencies). \n",
    "<br>\n",
    "The probability/weight for each of these topics within a document, along with the probability of a word being assigned to a topic, is assumed to begin with a Dirichlet probability distribution (*prior* in statistics), where the algorithm derives from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers like Blei and Ng proposed an idea by imagining how a machine that could only roll dice i.e. generate random numbers, could also write the documents in a corpus one may want to analyse. Given that one works with Bag of words during this process, it cuts out the part about sequencing such words together to make sense, to write a real document.\n",
    "<br>\n",
    "In this way, they modeled the statistics for the mix of words that would become a part of a particular BOW for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They envisioned a machine that only had two choices to make to process generating the mix of words for a specific document. The two roles of 'dice' represent:\n",
    "<br>\n",
    "1. The number of words to generate for the document (Poisson distribution)\n",
    "<br>\n",
    "2. The number of topics to mix together for the document (Dirichlet distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these two numbers, the difficult task is to choose the words for the document.\n",
    "<br>\n",
    "The imaginary BOW generating machine iterates over those topics and randomly chooses words appropriate to that topic until it hits the number of words that it had decided the document should contain in step 1.\n",
    "<br.\n",
    "Deciding the probabilities of those words for topics — the appropriateness of words for each topic— is the hard part. But once that has been determined, your 'bot' just looks up the probabilities for the words for each topic from a matrix of term-topic probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all this machine needs is a single parameter for that Poisson distribution (in the dice roll from step 1) that tells it what the 'average' document length should be, and a couple more parameters to define that Dirichlet distribution that sets up the number of topics.\n",
    "<br>\n",
    "Then your document generation algorithm needs a term-topic matrix of all the words and topics it likes to use, its vocabulary. And it needs a mix of topics that it likes to 'talk' about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this process more convenient in our use case, reversing the document generation (writing) problem back around to the original problem of estimating the topics and words from existing documents.\n",
    "<br>\n",
    "Initially, we need to measure or compute those parameters about words and topics for the first two steps.\n",
    "<br>\n",
    "Then we need to compute the term-topic matrix from a collection of documents. This is ultimately the process of LDiA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for steps can be computed by analysing the statistics of the documents in a corpus.\n",
    "<br>\n",
    "For instance, to solve step 1, one could calculate the mean number of words (n-grams) in all the BOW for the documents in a specified corpus.\n",
    "<br> \n",
    "An implementation in python may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nlpia.data.loaders import get_data\n",
    "pd.options.display.width = 120 \n",
    "sms = get_data('sms-spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corpus_len = 0 \n",
    "for document_text in sms.text:\n",
    "    total_corpus_len += len(casual_tokenize(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21.35"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "mean_document_len = total_corpus_len/len(sms)\n",
    "round(mean_document_len, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These document statistics are usually computed directly from the BOWs.\n",
    "<br>\n",
    "We need to make sure that we're counting the tokenized and vectorized text words with the specified documents.\n",
    "<br>\n",
    "It's also key to maintain a level of text normalization, particularly any stop word filtering, case folding etc. before counting up the unique terms.\n",
    "<br>\n",
    "This ensures that such count includes all the words in our BOW vector vocabulary (all the the n-grams we counting), but only those words that our BOW usa (i.e. not stop words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter we need to specify for an LDiA model is the **number of topics** which can be tedious.\n",
    "<br>\n",
    "This is the case because the number of topics in a particular set of documents can't be measured directly until after we've assigned words to such topics. \n",
    "<br>\n",
    "Clustering algorithms like ***K-means*** and ***KNN***, we would also need to pass the ***k*** parameter ahead of time.\n",
    "<br>\n",
    " We can guess the number of topics (analogous to the ***k*** in k-means, which is the number of clusters) and then check to see if that works for one's set of documents.\n",
    "<br>\n",
    "Once specified to the LDA model how many topics to look for, it will find the mix of words to put in each topic to optimise its objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimise this 'hyperparameter' (***k***, the number of topics) by adjusting it until it works for our application.\n",
    "<br>\n",
    "We can automate this optimisation if we can measure something about the quality of the LDiA language model for representing the meaning of the documents.\n",
    "<br>\n",
    "A 'cost function' to assess the optimisation for this model is how well/poorly the LDiA model performs in classification or regression problems such as sentiment analysis, document keyword tagging or topic analysis. We only need some labeled documents to test our topic model or classifier on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA topic model for SMS messages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics produced by LDiA tend to be more understandable and interpretable to humans.\n",
    "<br>\n",
    "This is the case as words that frequently occur together are assigned the same topic(s), which is expected by most humans. \n",
    "<br>\n",
    "Whereas LSA (PCA) tries to keep things spread apart to begin with, LDiA tries to keep things close together that started out close together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing LSA and LDiA, the math optimises for different things - based on the idea that the LDiA optimiser has a different objective function where it will reach a different objective.\n",
    "<br>\n",
    "To keep close high-dimensional vectors close together in the lower-dimensional space, LDiA has to twist and contort the space (and the vectors) in nonlinear ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment with an LDiA topic model, let's see how it works for a dataset of a few thousand SMS messages, labelled for spaminess.\n",
    "<br>\n",
    "Initially, we compute the word vector transformation and then some topic vectors for SMS message (document). We assume the use of 16 topics (components) to classify the spaminess of messages.\n",
    "<br>\n",
    "Keeping the number of topics (dimensions) low can help reduce the overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given LDiA works primarily with BOW count vectors rather than TF-IDF vectors - a workflow to compute BOW vectors in `sklearn` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "np.random.seed(42) # replicable results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_vector = counter.fit_transform(raw_documents=sms.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   0     1     2     3     4     5     6     7     8     9     ...  9222  9223  9224  9225  9226  9227  9228  9229  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0   \n2     0     0     0     0     0     0     0     1     1     1  ...     0     0     0     0     0     0     0     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0     0   \n\n   9230  9231  \n0     0     0  \n1     0     0  \n2     0     0  \n3     0     0  \n4     0     0  \n\n[5 rows x 9232 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>9222</th>\n      <th>9223</th>\n      <th>9224</th>\n      <th>9225</th>\n      <th>9226</th>\n      <th>9227</th>\n      <th>9228</th>\n      <th>9229</th>\n      <th>9230</th>\n      <th>9231</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 9232 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "bow_docs = pd.DataFrame(bow_vector.todense(), index=sms.index)\n",
    "bow_docs.head() # test vector transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(), counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}