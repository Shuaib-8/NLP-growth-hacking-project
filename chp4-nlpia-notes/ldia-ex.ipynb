{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDiA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Sematic Analysis (LSA) should be the first choice for most topic modelling, semantic search, or content-based recommendation engines.\n",
    "<br>\n",
    "Given the maths used for LSA is efficient and somewhat easy to follow, given that it produces a linear transformation that can be applied to new batches of natural language without training and little loss in accuracy. However, LDiA can give slightly better results in some situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDiA performs many things accomplished by topic models with LSA (and SVD under the hood), though unlike LSA, LDiA assumes a Dirichlet distribution of word frequencies. It's more precise about the statistics of allocating words to topics than the linear math of LSA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDiA creates a semantic vector space model (as with topic vectors) using a topic mix process that for any given document, such topics can be determined by the word mixtures in each topic by which topic those words were assigned to.\n",
    "<br>\n",
    "In some ways, this makes an LDiA topic model easier to understand, as words assigned to topics and topics assigned to documents tend to make more sense than for LSA.\n",
    "<br>\n",
    "LDiA assumes that each document is a mixture (linear combination) of some arbitrary number of topics that one can select when beginning to train the LDiA model. LDiA also assumes that each topic can be represented by a distribution of words (term frequencies). \n",
    "<br>\n",
    "The probability/weight for each of these topics within a document, along with the probability of a word being assigned to a topic, is assumed to begin with a Dirichlet probability distribution (*prior* in statistics), where the algorithm derives from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Researchers like Blei and Ng proposed an idea by imagining how a machine that could only roll dice i.e. generate random numbers, could also write the documents in a corpus one may want to analyse. Given that one works with Bag of words during this process, it cuts out the part about sequencing such words together to make sense, to write a real document.\n",
    "<br>\n",
    "In this way, they modeled the statistics for the mix of words that would become a part of a particular BOW for each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They envisioned a machine that only had two choices to make to process generating the mix of words for a specific document. The two roles of 'dice' represent:\n",
    "<br>\n",
    "1. The number of words to generate for the document (Poisson distribution)\n",
    "<br>\n",
    "2. The number of topics to mix together for the document (Dirichlet distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these two numbers, the difficult task is to choose the words for the document.\n",
    "<br>\n",
    "The imaginary BOW generating machine iterates over those topics and randomly chooses words appropriate to that topic until it hits the number of words that it had decided the document should contain in step 1.\n",
    "<br.\n",
    "Deciding the probabilities of those words for topics — the appropriateness of words for each topic— is the hard part. But once that has been determined, your 'bot' just looks up the probabilities for the words for each topic from a matrix of term-topic probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all this machine needs is a single parameter for that Poisson distribution (in the dice roll from step 1) that tells it what the 'average' document length should be, and a couple more parameters to define that Dirichlet distribution that sets up the number of topics.\n",
    "<br>\n",
    "Then your document generation algorithm needs a term-topic matrix of all the words and topics it likes to use, its vocabulary. And it needs a mix of topics that it likes to 'talk' about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make this process more convenient in our use case, reversing the document generation (writing) problem back around to the original problem of estimating the topics and words from existing documents.\n",
    "<br>\n",
    "Initially, we need to measure or compute those parameters about words and topics for the first two steps.\n",
    "<br>\n",
    "Then we need to compute the term-topic matrix from a collection of documents. This is ultimately the process of LDiA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters for steps can be computed by analysing the statistics of the documents in a corpus.\n",
    "<br>\n",
    "For instance, to solve step 1, one could calculate the mean number of words (n-grams) in all the BOW for the documents in a specified corpus.\n",
    "<br> \n",
    "An implementation in python may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from nltk.tokenize import casual_tokenize\n",
    "from nlpia.data.loaders import get_data\n",
    "pd.options.display.width = 120 \n",
    "sms = get_data('sms-spam')\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms.index = index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corpus_len = 0 \n",
    "for document_text in sms.text:\n",
    "    total_corpus_len += len(casual_tokenize(document_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "21.35"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "mean_document_len = total_corpus_len/len(sms)\n",
    "round(mean_document_len, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These document statistics are usually computed directly from the BOWs.\n",
    "<br>\n",
    "We need to make sure that we're counting the tokenized and vectorized text words with the specified documents.\n",
    "<br>\n",
    "It's also key to maintain a level of text normalization, particularly any stop word filtering, case folding etc. before counting up the unique terms.\n",
    "<br>\n",
    "This ensures that such count includes all the words in our BOW vector vocabulary (all the the n-grams we counting), but only those words that our BOW usa (i.e. not stop words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second parameter we need to specify for an LDiA model is the **number of topics** which can be tedious.\n",
    "<br>\n",
    "This is the case because the number of topics in a particular set of documents can't be measured directly until after we've assigned words to such topics. \n",
    "<br>\n",
    "Clustering algorithms like ***K-means*** and ***KNN***, we would also need to pass the ***k*** parameter ahead of time.\n",
    "<br>\n",
    " We can guess the number of topics (analogous to the ***k*** in k-means, which is the number of clusters) and then check to see if that works for one's set of documents.\n",
    "<br>\n",
    "Once specified to the LDA model how many topics to look for, it will find the mix of words to put in each topic to optimise its objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimise this 'hyperparameter' (***k***, the number of topics) by adjusting it until it works for our application.\n",
    "<br>\n",
    "We can automate this optimisation if we can measure something about the quality of the LDiA language model for representing the meaning of the documents.\n",
    "<br>\n",
    "A 'cost function' to assess the optimisation for this model is how well/poorly the LDiA model performs in classification or regression problems such as sentiment analysis, document keyword tagging or topic analysis. We only need some labeled documents to test our topic model or classifier on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA topic model for SMS messages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics produced by LDiA tend to be more understandable and interpretable to humans.\n",
    "<br>\n",
    "This is the case as words that frequently occur together are assigned the same topic(s), which is expected by most humans. \n",
    "<br>\n",
    "Whereas LSA (PCA) tries to keep things spread apart to begin with, LDiA tries to keep things close together that started out close together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing LSA and LDiA, the math optimises for different things - based on the idea that the LDiA optimiser has a different objective function where it will reach a different objective.\n",
    "<br>\n",
    "To keep close high-dimensional vectors close together in the lower-dimensional space, LDiA has to twist and contort the space (and the vectors) in nonlinear ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment with an LDiA topic model, let's see how it works for a dataset of a few thousand SMS messages, labelled for spaminess.\n",
    "<br>\n",
    "Initially, we compute the word vector transformation and then some topic vectors for SMS message (document). We assume the use of 16 topics (components) to classify the spaminess of messages.\n",
    "<br>\n",
    "Keeping the number of topics (dimensions) low can help reduce the overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given LDiA works primarily with BOW count vectors rather than TF-IDF vectors - a workflow to compute BOW vectors in `sklearn` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "np.random.seed(42) # replicable results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_vector = counter.fit_transform(raw_documents=sms.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       0     1     2     3     4     5     6     7     8     9     ...  9222  9223  9224  9225  9226  9227  9228  \\\nsms0      0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0   \nsms1      0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0   \nsms2!     0     0     0     0     0     0     0     1     1     1  ...     0     0     0     0     0     0     0   \nsms3      0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0   \nsms4      0     0     0     0     0     0     0     0     0     0  ...     0     0     0     0     0     0     0   \n\n       9229  9230  9231  \nsms0      0     0     0  \nsms1      0     0     0  \nsms2!     0     0     0  \nsms3      0     0     0  \nsms4      0     0     0  \n\n[5 rows x 9232 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>9222</th>\n      <th>9223</th>\n      <th>9224</th>\n      <th>9225</th>\n      <th>9226</th>\n      <th>9227</th>\n      <th>9228</th>\n      <th>9229</th>\n      <th>9230</th>\n      <th>9231</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sms0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sms1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sms2!</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sms3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sms4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 9232 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "bow_docs = pd.DataFrame(bow_vector.todense(), index=index)\n",
    "bow_docs.head() # test vector transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(), counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the first message `sms0`, we can double check to ensure that the count from the model makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "sms.loc['sms0'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": ",            1\n..           1\n...          2\namore        1\navailable    1\nName: sms0, dtype: int64"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "bow_docs.loc['sms0'][bow_docs.loc['sms0'] > 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use LDiA as shown below to construct topic vectors for our SMS corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "ldia = LDA(n_components=16, learning_method='batch')\n",
    "# fit/transform steps for ldia takes slightly longer than PCA/SVD \n",
    "# esp for large number of topics/words in a corpus \n",
    "ldia_model = ldia.fit_transform(bow_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16, 9232)"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "ldia.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model has allocated your 9,232 words (terms) to 16 topics (components). Let’s take a look at the first few words and how they’re allocated to your 16 topics. \n",
    "<br>\n",
    "Keep in mind that the counts and topics can differ based on each iteration. LDiA is a stochastic algorithm that relies on the random number generator to make some of the statistical decisions it has to make about allocating words to topics. \n",
    "<br>\n",
    "So the topic-word weights will be different from those shown, but they should have similar magnitudes. Each time we run `LatentDirichletAllocation` within `sklearn` (or any LDiA algorithm), we will get different results unless the **random seed** is set to a fixed value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "columns = [f'topic{i}' for i in range(ldia.n_components)]\n",
    "components = pd.DataFrame(ldia.components_.T, index=terms, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n!      184.03   15.00   72.22  394.95   45.48   36.14    9.55   44.81   \n\"        0.68    4.22    2.41    0.06  152.35    0.06    0.06    0.06   \n#        0.06    0.06    0.06    0.06    0.06    2.07    0.06    0.06   \n#150     0.06    0.06    0.06    0.06    0.06    0.06    0.06    0.06   \n#5000    0.06    0.06    0.06    0.06    0.06    0.06    0.06    0.06   \n\n       topic8  topic9  topic10  topic11  topic12  topic13  topic14  \\\n!        0.43   90.23    37.42    44.18    64.40   297.29    41.16   \n\"        0.45    0.68     8.42    11.42     0.07    62.72    12.27   \n#        0.06    0.06     0.06     0.06     1.07     4.05     0.06   \n#150     1.06    0.06     0.06     0.06     0.06     0.06     0.06   \n#5000    0.06    3.06     0.06     0.06     0.06     0.06     0.06   \n\n       topic15  \n!        11.70  \n\"         0.06  \n#         0.06  \n#150      0.06  \n#5000     0.06  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic0</th>\n      <th>topic1</th>\n      <th>topic2</th>\n      <th>topic3</th>\n      <th>topic4</th>\n      <th>topic5</th>\n      <th>topic6</th>\n      <th>topic7</th>\n      <th>topic8</th>\n      <th>topic9</th>\n      <th>topic10</th>\n      <th>topic11</th>\n      <th>topic12</th>\n      <th>topic13</th>\n      <th>topic14</th>\n      <th>topic15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>!</th>\n      <td>184.03</td>\n      <td>15.00</td>\n      <td>72.22</td>\n      <td>394.95</td>\n      <td>45.48</td>\n      <td>36.14</td>\n      <td>9.55</td>\n      <td>44.81</td>\n      <td>0.43</td>\n      <td>90.23</td>\n      <td>37.42</td>\n      <td>44.18</td>\n      <td>64.40</td>\n      <td>297.29</td>\n      <td>41.16</td>\n      <td>11.70</td>\n    </tr>\n    <tr>\n      <th>\"</th>\n      <td>0.68</td>\n      <td>4.22</td>\n      <td>2.41</td>\n      <td>0.06</td>\n      <td>152.35</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.45</td>\n      <td>0.68</td>\n      <td>8.42</td>\n      <td>11.42</td>\n      <td>0.07</td>\n      <td>62.72</td>\n      <td>12.27</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>#</th>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>2.07</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>1.07</td>\n      <td>4.05</td>\n      <td>0.06</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>#150</th>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>1.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n    </tr>\n    <tr>\n      <th>#5000</th>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>3.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n      <td>0.06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "components.round(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance, the '!' term occurs across all topics, but is a particularly strong part of `topic3`, where as something like '\"' puncuation mark is rarely occuring.\n",
    "<br>\n",
    "An inference surrounding `topic3` could be that its general theme is about emotional intensity or emphasis and doesn't care much about numbers/quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "!       394.952246\n.       218.049724\nto      119.533134\nu       118.857546\ncall    111.948541\n£       107.358914\n,        96.954384\n*        90.314783\nyour     90.215961\nis       75.750037\nName: topic3, dtype: float64"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "# exploring the top ten tokens for topic3 \n",
    "components.topic3.sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the top ten tokens for 'topic3' seem to be focussed around emphatic (imperative) directives requesting someone to do or pay something.\n",
    "<br>\n",
    "Also, it would be interesting to find out if this topic is used more often in spam messages as opposed to nonspam messages.\n",
    "<br>\n",
    "We can see the allocation of words to topics can be rationalised or reasoned about - even with this quick look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit an LDA classifier, we need to compute these LDiA topic vectors for all the corresponding documents (SMS messages). And let’s see how they are different from the topic vectors produced by SVD and PCA for those same documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\nsms0     0.00    0.62    0.00    0.00    0.00    0.00    0.00    0.00   \nsms1     0.01    0.01    0.01    0.01    0.01    0.01    0.01    0.01   \nsms2!    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \nsms3     0.00    0.00    0.00    0.00    0.09    0.00    0.00    0.00   \nsms4     0.39    0.00    0.33    0.00    0.00    0.00    0.14    0.00   \n\n       topic8  topic9  topic10  topic11  topic12  topic13  topic14  \\\nsms0     0.34    0.00     0.00     0.00     0.00     0.00     0.00   \nsms1     0.78    0.01     0.01     0.12     0.01     0.01     0.01   \nsms2!    0.00    0.98     0.00     0.00     0.00     0.00     0.00   \nsms3     0.85    0.00     0.00     0.00     0.00     0.00     0.00   \nsms4     0.00    0.00     0.00     0.00     0.09     0.00     0.00   \n\n       topic15  \nsms0      0.00  \nsms1      0.01  \nsms2!     0.00  \nsms3      0.00  \nsms4      0.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic0</th>\n      <th>topic1</th>\n      <th>topic2</th>\n      <th>topic3</th>\n      <th>topic4</th>\n      <th>topic5</th>\n      <th>topic6</th>\n      <th>topic7</th>\n      <th>topic8</th>\n      <th>topic9</th>\n      <th>topic10</th>\n      <th>topic11</th>\n      <th>topic12</th>\n      <th>topic13</th>\n      <th>topic14</th>\n      <th>topic15</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sms0</th>\n      <td>0.00</td>\n      <td>0.62</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.34</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>sms1</th>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.78</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.12</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <th>sms2!</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.98</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>sms3</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.09</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.85</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>sms4</th>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.33</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.14</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.09</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "ldia16_topic_vectors = pd.DataFrame(ldia_model, index=index, columns=columns)\n",
    "ldia16_topic_vectors.round(2).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these topics are more cleanly separated among each document. \n",
    "<br>\n",
    "There are a lot of zeros in the allocation of topics to messages. \n",
    "<br>\n",
    "This is one of the things that makes LDiA topics easier to explain to peers or less technical folk when making business decisions based on the NLP pipeline results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA/LDA - spam classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test how good LDiA topics are at predicting labels such as 'spam', we'll use the LDiA topic vectors to train an LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.94"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ldia16_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "# Can make predictions based on whole dataset of sms messages \n",
    "sms['ldia16_spam'] = lda.predict(ldia16_topic_vectors)\n",
    "round(float(lda.score(X_test, y_test)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithms for `train_test_split()` and LDiA are stochastic. So each time we run it we will get different results and different accuracy values. \n",
    "<br>\n",
    "If we want to make your pipeline repeatable, look for the seed argument for these models and dataset splitters. You can set the seed to the same value with each run to get reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way a 'collinear' warning can occur is if your text has a few 2-grams or 3-grams where their component words only ever occur together. So the resulting LDiA model had to arbitrarily split the weights among these equivalent term frequencies. Can you find the words in your SMS messages that are causing this 'collinearity' (zero determinant)?\n",
    "<br>\n",
    "We’re looking for a word that, whenever it occurs, another word (its pair) is always in the same message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this search with Python rather than by hand.\n",
    "<br> \n",
    "First, we probably just want to look for any identical bag-of-words vectors in your corpus. These could occur for SMS messages that aren’t identical, like 'Hi there Bob!' or 'Bob, Hi there', because they have the same word counts. \n",
    "<br>\n",
    "We can iterate through all the pairings of the bags of words to look for identical vectors. These will definitely cause a 'collinearity' warning in either **LDiA** or **LSA**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don’t find any exact **BOW** vector duplicates, we could iterate through all the pairings of the words in our vocabulary. \n",
    "<br>\n",
    "We would then iterate through all the bags of words to look for the pairs of SMS messages that contain those exact same two words. If there aren’t any times that those words occur separately in the SMS messages, you’ve found one of the 'collinearities' in your dataset. \n",
    "<br>\n",
    "Some common 2-grams that might cause this are the first and last names of famous people that always occur together and are never used separately, like 'Bill Gates' (as long as there are no other Bills in your SMS messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got more than 90% accuracy on your test set, and we only had to train on half your available data. \n",
    "<br>\n",
    "But we did get a (possible) warning about your features being collinear due to your limited dataset, which gives LDA an “under-determined” problem. The determinant of your topic-document matrix is close to zero, once we discard half the documents with `train_test_split`.\n",
    "<br>\n",
    "If you ever need to, we can turn down the LDiA `n_components` to 'fix' this issue, but it would tend to combine those topics together that are a linear combination of each other (collinear)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how such LDiA model compares to a higher dimensional model based on the TF-IDF vectors.\n",
    "<br>\n",
    "The TF-IDF vectors have many more features (>3k unique terms) - which is likely to suffer from the problems of poor generalization and overfitting.\n",
    "<br>\n",
    "Hence, the intermediary generalisation step of LDiA/PCA should help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.748"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs, sms.spam.values, test_size=0.5, random_state=271828)\n",
    "# Assume only 'one topic' given we're only interested in a score (classification) for spam topic \n",
    "lda = LDA(n_components=1)\n",
    "# Fitting LDA model to very much many features will take time\n",
    "lda = lda.fit(X_train, y_train)\n",
    "round(float(lda.score(X_test, y_test)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy is worse than when we trained it on lower-dimensional topic vectors - instead of TF-IDF vectors.\n",
    "<br>\n",
    "This is the purpose of what topic modelling (LSA) is supposed to do. It helps us generalise our models from a small training set, where it still works well on messages using different combinations of words (but similar topics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDiA - increasing topics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we'll perform another run with more dimensions (topics). Increasing dimensions while using a LDiA model may compensate for a lack of performance relative to LSA (PCA), so we'll need more topics to allocate words to. Here, we'll try 32 topics (components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(32, 9232)"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "ldia32 = LDiA(n_components=32, learning_method='batch') # batch is default\n",
    "ldia32_topic_vectors = ldia32.fit_transform(bow_docs)\n",
    "ldia32.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this vector transformation, we can compute the corresponding 32-D topic vectors for all the documents (SMS messages) within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_32 = ['topic{}'.format(i) for i in range(ldia32.n_components)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\nsms0      0.0    0.00     0.0    0.06    0.14    0.00     0.0     0.0   \nsms1      0.0    0.00     0.0    0.00    0.53    0.00     0.0     0.0   \nsms2!     0.0    0.00     0.0    0.00    0.00    0.65     0.0     0.0   \nsms3      0.0    0.11     0.0    0.00    0.39    0.00     0.0     0.0   \nsms4      0.0    0.00     0.0    0.00    0.00    0.00     0.0     0.0   \n\n       topic8  topic9  ...  topic22  topic23  topic24  topic25  topic26  \\\nsms0      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \nsms1      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \nsms2!     0.0     0.0  ...      0.0      0.0     0.00     0.33      0.0   \nsms3      0.0     0.0  ...      0.0      0.0     0.00     0.00      0.0   \nsms4      0.0     0.0  ...      0.0      0.0     0.09     0.00      0.0   \n\n       topic27  topic28  topic29  topic30  topic31  \nsms0      0.00      0.0     0.00      0.0      0.0  \nsms1      0.00      0.0     0.14      0.0      0.0  \nsms2!     0.00      0.0     0.00      0.0      0.0  \nsms3      0.00      0.0     0.00      0.0      0.0  \nsms4      0.47      0.0     0.00      0.0      0.0  \n\n[5 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic0</th>\n      <th>topic1</th>\n      <th>topic2</th>\n      <th>topic3</th>\n      <th>topic4</th>\n      <th>topic5</th>\n      <th>topic6</th>\n      <th>topic7</th>\n      <th>topic8</th>\n      <th>topic9</th>\n      <th>...</th>\n      <th>topic22</th>\n      <th>topic23</th>\n      <th>topic24</th>\n      <th>topic25</th>\n      <th>topic26</th>\n      <th>topic27</th>\n      <th>topic28</th>\n      <th>topic29</th>\n      <th>topic30</th>\n      <th>topic31</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sms0</th>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.06</td>\n      <td>0.14</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>sms1</th>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.53</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.14</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>sms2!</th>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.65</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.33</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>sms3</th>\n      <td>0.0</td>\n      <td>0.11</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>sms4</th>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.09</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.47</td>\n      <td>0.0</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors, index=index, columns=columns_32)\n",
    "ldia32_topic_vectors.round(2).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, these topics (32) are even more sparse - also being more cleanly separated.\n",
    "<br>\n",
    "Afterwards, we can utilise the LDA model (classifier) to train upon these spam messages, utilising the added 32D LDiA topic vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2418, 32)"
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ldia32_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia32_spam'] = lda.predict(ldia32_topic_vectors)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.936"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "round(float(lda.score(X_test, y_test)),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accuracy scores, 93.6% is comparable to the 94% score retrieved from the 16-D LDiA topic vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that 'warnings' related to collinearity issues has nothing to do with opitmisation of topics (components) under a specified topic model.\n",
    "<br>\n",
    "Hence, collinearity is an inherent problem within the underlying data. \n",
    "<br>\n",
    "To solve such a problem, we need to add 'noise' or metadata to the SMS data as synthetic words, or remove those duplicate word vectors/pairings that happen to repeat frequently in our documents.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger number of topics allows for more precision about the topics and generally produce topics that linearly separate better.\n",
    "<br>\n",
    "Yet, the performance of `LDiA + LDA` is not quite as good as the 96% accuracy of `PCA + LDA`.\n",
    "<br>\n",
    "Hence, PCA is keeping our SMS topic vectors spread out more efficiently - allowing for a wider gap between messages to cut with a hyperplane (decision boundary) to separate classes.\n",
    "<br>\n",
    "Finding explainable topics, like those used for summarisation, is what `LDiA` is good at. Also, it can do a fairly good job at creating topics useful for linear classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}