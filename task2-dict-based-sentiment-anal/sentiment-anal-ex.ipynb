{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one of initial stages in NLP pipeline through tokenization (including text normalization, n-grams, stems, lemmas), such tokens contain information around a word's sentiment i.e. emotion or feeling a word invokes. \n",
    "* ***Sentiment analysis*** -  measuring the sentiment of phrases or chunks of text\n",
    "* Examples - Companies such as Movie review sites/Amazon request feedback on their services or the products they promote within the market place\n",
    "    * Star rating - typically from 1-5 gives us quantitative data about how people feel about products they've purchased or services they've used\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The possibility of a machine algorithm detecting sentiment is crucial, especially when humans (unless they have superior domain knowledge) can be erroneous in retrieving a non-biased sentiment score for a rating (particularly if it's negative). The ability of input that represents natural language text helps us retrieve and extract information from it. Given the Big Data era, NLP pipelines can process large amounts of text fairly quickly and objectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two approaches to sentiment analysis \n",
    "\n",
    "1) Rules-based algorithm composed by a human \n",
    "<br>\n",
    "2) Machine learning (ML) model learned from data by a machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Rules based** - such approach uses human constructed rules of thumb (heuristics) to measure sentiment. A common rule-based approach to sentiment analysis is to find specific keywords in the corpus and map each one to numerical scores or weights in a dictionary/mapping. Such step builds upon the tokenization process. The final step in computing this rule is to add up the score for each keyword in a document that could also be found in dictionary of sentiment scores. The final score is based on polarity scheme (-1 for absolutuely negative; 0 for neutral; +1 for absolutuely positive).\n",
    "* **Machine learning (supervised learning)** - relies on labeled set of data documents to train a ML model to create such rules. The ML sentiment model is trained to process input text and output a numerical value (score) for sentiment being measured such as **positivity, negativy or spaminess**. A lot of labeled data with the right sentiment score is required. Hence, utilise a KPI such as a star rating to then get a corresponding set of keywords associated with that star rating to come up with a labelled output (target variable) to state either **positive** or **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) VADER - rules-based sentiment analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valence Aware Dictionary for (s)Entiment Reasoning (VADER) is one of the common rules-based sentiment analyser algorithms. NLTK contains an implementation of this under `nltk.sentiment.vader`, but one of the creators **Hutto (GA Tech)** maintains the distinctive (his own) python package `vaderSentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(\"( '}{' )\", 1.6),\n (\"can't stand\", -2.0),\n ('fed up', -1.8),\n ('screwed up', -1.5)]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sent_anal = SentimentIntensityAnalyzer()\n",
    "lexicon = sent_anal.lexicon\n",
    "# Only retrieve phrases with empty 'whitespace' between i.e. n-grams/bigrams\n",
    "[(tok, score) for tok, score in lexicon.items() if \" \" in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404}\n{'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.3724}\n"
    }
   ],
   "source": [
    "# Computing polarity scores for example texts \n",
    "print(sent_anal.polarity_scores(text='Python is handy and is good for when we need to use NLP'))\n",
    "print(sent_anal.polarity_scores(text='Python is not a poor choice for implementing most applications'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VADER algorithm considers the concentration of sentiment polarity in three separate scores \n",
    "<br>\n",
    "1) **Positive**\n",
    "<br>\n",
    "2) **Neutral**\n",
    "<br>\n",
    "3) **Negative**\n",
    "<br>\n",
    "<br>\n",
    "Then combines them together into a **compound** positivity sentiment.\n",
    "<br>\n",
    "VADER even manages to handle negation fairly well by taking into account 'not a poor' by considering it in a slightly positive context like the case with 'good' i.e. through neighbouring associations rather than in isolation.\n",
    "<br>\n",
    "VADER's inherent tokenization doesn't consider any words that aren't in its lexicon/vocabulary along with n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+0.9281: Amazingly perfect! Nice one! :) :)\n-0.8856: Completely horrible! The product is useless. :@\n+0.4404: The food was decent. some good and bads meals in between.\n"
    }
   ],
   "source": [
    "corpus = ['Amazingly perfect! Nice one! :) :)', 'Completely horrible! The product is useless. :@',\n",
    "'The food was decent. some good and bads meals in between.']\n",
    "for doc in corpus:\n",
    "    scores = sent_anal.polarity_scores(doc)\n",
    "    print(f\"{scores['compound']:+}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only drawback to VADER is that it contains only 7500 words in its lexicon i.e. it can only look at that many possible words that could be included in one's own corpus.\n",
    "<br>\n",
    "Analysing larger copora would mean having to understand all the words in the lexicon and possibly adding the scores (polarity) to put in the lexicon i.e. supplementing `sent_anal.lexicon`.\n",
    "<br>\n",
    "ML (Naive Bayes model) will help us out with such labelling on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Naive Bayes - ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any standard ML model, it's always important to identify the feature(s) and target variable.\n",
    "* **Feature** - text i.e. all the words in our corpus\n",
    "* **Target** - Sentiment label\n",
    "\n",
    "**Naive Bayes** attempts to find keywords in our lexicon that are predictive of our sentiment (output) label.\n",
    "<br>\n",
    "The model will compute the internal coefficients to map words/tokens to score thresholds that fall onto a sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "sentiment                                               text\nid                                                              \n1        2.27  The Rock is destined to be the 21st Century's ...\n2        3.53  The gorgeously elaborate continuation of ''The...\n3       -0.60                     Effective but too tepid biopic\n4        1.47  If you sometimes like to go to the movies to h...\n5        1.73  Emerges as something rare, an issue movie that...\n(10605, 2)\n"
    }
   ],
   "source": [
    "# Need to be in nlpiaenv (conda) virtual environment before running this cell: conda activate nlpiaenv\n",
    "from nlpia.data.loaders import get_data\n",
    "movies = get_data('hutto_movies')\n",
    "print(movies.head().round(2))\n",
    "print(movies.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       sentiment\ncount   10605.00\nmean        0.00\nstd         1.92\nmin        -3.88\n25%        -1.77\n50%        -0.08\n75%         1.83\nmax         3.94",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10605.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.92</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-3.88</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.77</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.08</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.83</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.94</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# It's wise to check on summary statistics such as range of the sentiment scores\n",
    "movies.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import Counter\n",
    "from nltk.tokenize import casual_tokenize # Better at handling slang, usernames and puncuation altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow: bag-of-words\n",
    "bow = [] \n",
    "for text in movies['text']:\n",
    "    bow.append(Counter(casual_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bow = pd.DataFrame.from_records(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each word vector has missing values filled with 0 ('false') for token appearing in such sentence\n",
    "#df_bow.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(10605, 20756)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# casting as int after filling in missing values helps with display and enables compression of memory for the DataFrame\n",
    "df_bow = df_bow.fillna(0).astype(int)\n",
    "df_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow DataFrame to show wide output (many columns) easier\n",
    "pd.set_option('display.width', 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n0    1     1   1         1   2   1    1     1          1    1  ...    0   \n1    2     0   1         0   0   0    1     0          0    0  ...    0   \n2    0     0   0         0   0   0    0     0          0    0  ...    0   \n3    0     0   1         0   4   0    1     0          0    0  ...    0   \n4    0     0   0         0   0   0    0     0          0    0  ...    0   \n\n   slummer  Rashomon  dipsticks  Bearable  Staggeringly  ’  ve  \\\n0        0         0          0         0             0  0   0   \n1        0         0          0         0             0  0   0   \n2        0         0          0         0             0  0   0   \n3        0         0          0         0             0  0   0   \n4        0         0          0         0             0  0   0   \n\n   muttering  dissing  \n0          0        0  \n1          0        0  \n2          0        0  \n3          0        0  \n4          0        0  \n\n[5 rows x 20756 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>The</th>\n      <th>Rock</th>\n      <th>is</th>\n      <th>destined</th>\n      <th>to</th>\n      <th>be</th>\n      <th>the</th>\n      <th>21st</th>\n      <th>Century's</th>\n      <th>new</th>\n      <th>...</th>\n      <th>Ill</th>\n      <th>slummer</th>\n      <th>Rashomon</th>\n      <th>dipsticks</th>\n      <th>Bearable</th>\n      <th>Staggeringly</th>\n      <th>’</th>\n      <th>ve</th>\n      <th>muttering</th>\n      <th>dissing</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 20756 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Here is clear to see that vocabulary normalization would be of benefit by limiting the number of columns to be observed (dimensions)\n",
    "# For now carry on as this is just a example run through of NLP sentiment analysis pipeline through ML\n",
    "df_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  \\\n0    1     1   1         1   2   1    1     1          1    1  ...   \n1    2     0   1         0   0   0    1     0          0    0  ...   \n2    0     0   0         0   0   0    0     0          0    0  ...   \n3    0     0   1         0   4   0    1     0          0    0  ...   \n4    0     0   0         0   0   0    0     0          0    0  ...   \n\n   Schwarzenegger  ,  Jean  Claud  Van  Damme  or  Steven  Segal  .  \n0               1  1     1      1    1      1   1       1      1  1  \n1               0  0     0      0    0      0   0       0      0  4  \n2               0  0     0      0    0      0   0       0      0  0  \n3               0  1     0      0    0      0   0       0      0  1  \n4               0  1     0      0    0      0   0       0      0  1  \n\n[5 rows x 33 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>The</th>\n      <th>Rock</th>\n      <th>is</th>\n      <th>destined</th>\n      <th>to</th>\n      <th>be</th>\n      <th>the</th>\n      <th>21st</th>\n      <th>Century's</th>\n      <th>new</th>\n      <th>...</th>\n      <th>Schwarzenegger</th>\n      <th>,</th>\n      <th>Jean</th>\n      <th>Claud</th>\n      <th>Van</th>\n      <th>Damme</th>\n      <th>or</th>\n      <th>Steven</th>\n      <th>Segal</th>\n      <th>.</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 33 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Test to see first sentence within corpus bow representation\n",
    "df_bow.head()[list(bow[0].keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "# fit X - df of bow\n",
    "# fit y - for now movie sentiment filtered only for positives instances (sentiment > 0)\n",
    "nb = nb.fit(df_bow, movies.sentiment > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing where classes are True (i.e. second column of array which is 1 under python indexing)\n",
    "movies['predicted_sentiment'] = nb.predict_proba(df_bow)[:,1] * 8 - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Mean Absolute Error (MAE) is 1.9\n"
    }
   ],
   "source": [
    "movies['error'] = (movies['predicted_sentiment'] - movies['sentiment']).abs()\n",
    "mae = movies['error'].mean()\n",
    "# print out MAE to one decimal place\n",
    "print(f'the Mean Absolute Error (MAE) is {mae:0.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['sentiment_ispositive'] = (movies['sentiment'] > 0).astype(int)\n",
    "movies['predicted_ispositive'] = (movies['predicted_sentiment'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    sentiment  predicted_sentiment  sentiment_ispositive  \\\nid                                                         \n1    2.266667             2.511515                     1   \n2    3.533333             3.999904                     1   \n3   -0.600000            -3.655976                     0   \n4    1.466667             1.940954                     1   \n5    1.733333             3.910373                     1   \n6    2.533333             3.995188                     1   \n7    2.466667             3.960466                     1   \n8    1.266667            -1.918701                     1   \n\n    predicted_ispositive  \nid                        \n1                      1  \n2                      1  \n3                      0  \n4                      1  \n5                      1  \n6                      1  \n7                      1  \n8                      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>predicted_sentiment</th>\n      <th>sentiment_ispositive</th>\n      <th>predicted_ispositive</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2.266667</td>\n      <td>2.511515</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.533333</td>\n      <td>3.999904</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.600000</td>\n      <td>-3.655976</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.466667</td>\n      <td>1.940954</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.733333</td>\n      <td>3.910373</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.533333</td>\n      <td>3.995188</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.466667</td>\n      <td>3.960466</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.266667</td>\n      <td>-1.918701</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "movies['''sentiment predicted_sentiment sentiment_ispositive\\\n",
    "    predicted_ispositive'''.split()].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9344648750589345"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# check accuracy of the model \n",
    "(movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9344648750589345"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# Using sklearn accuracy score function instead of filtering/computation version directly above\n",
    "from sklearn.metrics import accuracy_score\n",
    "# (y_true, y_pred) - bear in mind no test sets\n",
    "accuracy_score(movies['sentiment_ispositive'], movies['predicted_ispositive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a convenient process at building a sentiment analyser with relatively minimal code and a large set of labelled/text data. This can be better at computation over large sets compared to VADER with a compiled list of 7500 words and their sentiment.\n",
    "<br>\n",
    "Some steps to refine model:\n",
    "* Split training data -  Leave training data for model to learn and place a subset out to test for out of sample accuracy (test how well model predicts on new data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = get_data('hutto_products')\n",
    "bow = [] \n",
    "for text in products['text']:\n",
    "    bow.append(Counter(casual_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   troubleshooting  ad  -  2500  and  2600  no  picture  scrolling  b  \\\n0                1   2  2     1    1     1   1        1          1  1   \n1                0   0  0     0    0     0   0        0          0  0   \n2                0   0  0     0    0     0   0        0          0  0   \n3                0   0  0     0    2     0   0        0          1  0   \n4                1   0  0     0    0     0   0        0          0  0   \n\n   ...  undone  warrranty  expire  expired  voids  develops  soldier  \\\n0  ...       0          0       0        0      0         0        0   \n1  ...       0          0       0        0      0         0        0   \n2  ...       0          0       0        0      0         0        0   \n3  ...       0          0       0        0      0         0        0   \n4  ...       0          0       0        0      0         0        0   \n\n   serving  baghdad  harddisk  \n0        0        0         0  \n1        0        0         0  \n2        0        0         0  \n3        0        0         0  \n4        0        0         0  \n\n[5 rows x 5687 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>troubleshooting</th>\n      <th>ad</th>\n      <th>-</th>\n      <th>2500</th>\n      <th>and</th>\n      <th>2600</th>\n      <th>no</th>\n      <th>picture</th>\n      <th>scrolling</th>\n      <th>b</th>\n      <th>...</th>\n      <th>undone</th>\n      <th>warrranty</th>\n      <th>expire</th>\n      <th>expired</th>\n      <th>voids</th>\n      <th>develops</th>\n      <th>soldier</th>\n      <th>serving</th>\n      <th>baghdad</th>\n      <th>harddisk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 5687 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "df_product_bow = pd.DataFrame.from_records(bow)\n",
    "df_product_bow = df_product_bow.fillna(0).astype(int)\n",
    "df_product_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st',\n       'Century's', 'new',\n       ...\n       'sligtly', 'owner', '81', 'defectively', 'warrranty', 'expire',\n       'expired', 'voids', 'baghdad', 'harddisk'],\n      dtype='object', length=23302)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# Add data from two bow dataframes together based on rows \n",
    "# df_bow.append(df_product_bow)\n",
    "df_all_bow = pd.concat([df_bow, df_product_bow], axis='rows')\n",
    "df_all_bow.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New bow df **(df_all_bow)** has some tokens that weren't in the original bow df **(df_bow)** - 23302 columns now instead of 20756."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3546, 20756)\n(10605, 20756)\n"
    }
   ],
   "source": [
    "# product_bow at this stage of filtering for some reason reverts back to filling NANs instead of 0 as \n",
    "# Fix here just in case\n",
    "df_product_bow = df_all_bow.iloc[len(movies):][df_bow.columns].fillna(0).astype(int)\n",
    "print(df_product_bow.shape) # New product bow dataframe \n",
    "print(df_bow.shape) # Original bow dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code step ensures that new bow df has an identical number of columns (tokens) in the same order as the orignal df used to train the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['ispos'] = (products['sentiment'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making our predictions on unseen data here - i.e. product reviews \n",
    "products['predicted_ispos'] = nb.predict(df_product_bow).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    id  sentiment                                               text  \\\n0  1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...   \n1  1_2      -0.15  repost from january 13, 2004 with a better fit...   \n2  1_3      -0.20  does your apex dvd player only play dvd audio ...   \n3  1_4      -0.10  or does it play audio and video but scrolling ...   \n4  1_5      -0.50  before you try to return the player or waste h...   \n\n   ispos  predicted_ispos  \n0      0                0  \n1      0                0  \n2      0                0  \n3      0                0  \n4      0                0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>ispos</th>\n      <th>predicted_ispos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_1</td>\n      <td>-0.90</td>\n      <td>troubleshooting ad-2500 and ad-2600 no picture...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1_2</td>\n      <td>-0.15</td>\n      <td>repost from january 13, 2004 with a better fit...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1_3</td>\n      <td>-0.20</td>\n      <td>does your apex dvd player only play dvd audio ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1_4</td>\n      <td>-0.10</td>\n      <td>or does it play audio and video but scrolling ...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1_5</td>\n      <td>-0.50</td>\n      <td>before you try to return the player or waste h...</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5572476029328821"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "(products.predicted_ispos == products.ispos).sum() / len(products)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive bayes model predictions based on product reviews (positive reviews) performed poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Reason*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference between document text lexicons - vocabulary from the product texts has 2546 (23302 - 20756) tokens that were not present in the movie reviews (texts our model was originally trained on). This accounts to around 10% of words/tokens in the original movie reviews tokenization - where all those words will not have have any weights or scores in our Naive Bayes model\n",
    "* Potential Naive Bayes model consequences - Naive Bayes doesn't handle negation as well as VADER does. The additional step of including n-grams into our tokenizer to connect negation words (like 'not'/'never')to be used alongside valid positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}