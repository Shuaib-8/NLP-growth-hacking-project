{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one of initial stages in NLP pipeline through tokenization (including text normalization, n-grams, stems, lemmas), such tokens contain information around a word's sentiment i.e. emotion or feeling a word invokes. \n",
    "* ***Sentiment analysis*** -  measuring the sentiment of phrases or chunks of text\n",
    "* Examples - Companies such as Movie review sites/Amazon request feedback on their services or the products they promote within the market place\n",
    "    * Star rating - typically from 1-5 gives us quantitative data about how people feel about products they've purchased or services they've used\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The possibility of a machine algorithm detecting sentiment is crucial, especially when humans (unless they have superior domain knowledge) can be erroneous in retrieving a non-biased sentiment score for a rating (particularly if it's negative). The ability of input that represents natural language text helps us retrieve and extract information from it. Given the Big Data era, NLP pipelines can process large amounts of text fairly quickly and objectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two approaches to sentiment analysis \n",
    "\n",
    "1) Rules-based algorithm composed by a human \n",
    "<br>\n",
    "2) Machine learning (ML) model learned from data by a machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Rules based** - such approach uses human constructed rules of thumb (heuristics) to measure sentiment. A common rule-based approach to sentiment analysis is to find specific keywords in the corpus and map each one to numerical scores or weights in a dictionary/mapping. Such step builds upon the tokenization process. The final step in computing this rule is to add up the score for each keyword in a document that could also be found in dictionary of sentiment scores. The final score is based on polarity scheme (-1 for absolutuely negative; 0 for neutral; +1 for absolutuely positive).\n",
    "* **Machine learning (supervised learning)** - relies on labeled set of data documents to train a ML model to create such rules. The ML sentiment model is trained to process input text and output a numerical value (score) for sentiment being measured such as **positivity, negativy or spaminess**. A lot of labeled data with the right sentiment score is required. Hence, utilise a KPI such as a star rating to then get a corresponding set of keywords associated with that star rating to come up with a labelled output (target variable) to state either **positive** or **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) VADER - rules-based sentiment analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valence Aware Dictionary for (s)Entiment Reasoning (VADER) is one of the common rules-based sentiment analyser algorithms. NLTK contains an implementation of this under `nltk.sentiment.vader`, but one of the creators **Hutto (GA Tech)** maintains the distinctive (his own) python package `vaderSentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(\"( '}{' )\", 1.6),\n (\"can't stand\", -2.0),\n ('fed up', -1.8),\n ('screwed up', -1.5)]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "sent_anal = SentimentIntensityAnalyzer()\n",
    "lexicon = sent_anal.lexicon\n",
    "# Only retrieve phrases with empty 'whitespace' between i.e. n-grams/bigrams\n",
    "[(tok, score) for tok, score in lexicon.items() if \" \" in tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.4404}\n{'neg': 0.0, 'neu': 0.779, 'pos': 0.221, 'compound': 0.3724}\n"
    }
   ],
   "source": [
    "# Computing polarity scores for example texts \n",
    "print(sent_anal.polarity_scores(text='Python is handy and is good for when we need to use NLP'))\n",
    "print(sent_anal.polarity_scores(text='Python is not a poor choice for implementing most applications'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VADER algorithm considers the concentration of sentiment polarity in three separate scores \n",
    "<br>\n",
    "1) **Positive**\n",
    "<br>\n",
    "2) **Neutral**\n",
    "<br>\n",
    "3) **Negative**\n",
    "<br>\n",
    "<br>\n",
    "Then combines them together into a **compound** positivity sentiment.\n",
    "<br>\n",
    "VADER even manages to handle negation fairly well by taking into account 'not a poor' by considering it in a slightly positive context like the case with 'good' i.e. through neighbouring associations rather than in isolation.\n",
    "<br>\n",
    "VADER's inherent tokenization doesn't consider any words that aren't in its lexicon/vocabulary along with n-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+0.9281: Amazingly perfect! Nice one! :) :)\n-0.8856: Completely horrible! The product is useless. :@\n+0.4404: The food was decent. some good and bads meals in between.\n"
    }
   ],
   "source": [
    "corpus = ['Amazingly perfect! Nice one! :) :)', 'Completely horrible! The product is useless. :@',\n",
    "'The food was decent. some good and bads meals in between.']\n",
    "for doc in corpus:\n",
    "    scores = sent_anal.polarity_scores(doc)\n",
    "    print(f\"{scores['compound']:+}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Naive Bayes - ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to be in nlpiaenv (conda) virtual environment before running this cell: conda activate nlpiaenv\n",
    "import nlpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}