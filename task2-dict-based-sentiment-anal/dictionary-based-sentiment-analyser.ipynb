{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.4 64-bit ('base': conda)",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Creating a Dictionary-based Sentiment Analyzer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Given the small corpus (reviews textual dataset) generated in task 1, the objective is to construct a dictionary-based sentiment analyser.\n",
    "<br>\n",
    "<br>\n",
    "Some of the lessons taught during these tasks:\n",
    "* Word and sentence tokenization \n",
    "* Review score classification \n",
    "* Insights surrounding score and review comparisons \n",
    "* Correlation analysis surrounding review groups \n",
    "* Accounting for negation within sentiment analyser\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necessary imports needed for analysis \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os, pathlib\n",
    "import matplotlib.pyplot as plt \n",
    "import altair\n",
    "import pandas_bokeh\n",
    "%matplotlib inline "
   ]
  },
  {
   "source": [
    "### Load small corpus dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move back into task1 repo where review (small) corpus is located\n",
    "path = pathlib.Path().home()/'Desktop/nlp-map-project/task1-create-dataset'\n",
    "try:\n",
    "    path = os.chdir(path) \n",
    "except FileNotFoundError as a :\n",
    "    print('Already in directory/folder, carry on!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('small_corpus.csv')"
   ]
  },
  {
   "source": [
    "When dealing with NLP/ML problems, we must initially ensure that there are no missing values, otherwise this will lead to problems later down the line."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ratings    0\n",
       "reviews    4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# there are some missing reviews \n",
    "# which is not substanstial relative to size of dataset/corpus \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      ratings reviews\n",
       "686         1     NaN\n",
       "2590        4     NaN\n",
       "3197        5     NaN\n",
       "3470        5     NaN"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ratings</th>\n      <th>reviews</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>686</th>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2590</th>\n      <td>4</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3197</th>\n      <td>5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3470</th>\n      <td>5</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df[df['reviews'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaNs with empty string (whitespace) \n",
    "df['reviews'] = df['reviews'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to ensure there's no nulls \n",
    "assert df['reviews'].notna().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review_sample = df['reviews'].head().tolist()\n",
    "rating_sample = df['ratings'].head().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 1 1 1 1\nRecently UBISOFT had to settle a huge class-action suit brought against the company for bundling (the notoriously harmful) StarFORCE DRM with its released games. So what the geniuses at the helm do next? They decide to make the same mistake yet again - by choosing the same DRM scheme that made BIOSHOCK, MASS EFFECT and SPORE infamous: SecuROM 7.xx with LIMITED ACTIVATIONS!\n\nMASS EFFECT can be found in clearance bins only months after its release; SPORE not only undersold miserably but also made history as the boiling point of gamers lashing back, fed up with idiotic DRM schemes. And the clueless MBAs that run an art-form as any other commodity business decided that, \"hey, why not jump into THAT mud-pond ourselves?\"\n\nThe original FAR CRY was such a GREAT game that any sequel of it would have to fight an uphill battle to begin with (especially without its original developing team). Now imagine shooting this sequel on the foot with a well known, much hated and totally useless DRM scheme that turns it into another Rent-A-Game no one wants. Were I a UBISOFT stock-holder I would be ordering my broker to \"Sell-Sell!-SELL!!\" instead of posting this...\n\nEver since its 7.xx version, SecuROM has NOTHING to do with \"fighting piracy\". All it does in this direction (blocking certain optical and virtual drives) is a very old, lame and already bypassed attempt that serves as a thin smoke-screen. SecuROM is, in fact, an intruding and silent Data-Miner and Root-Hijacker that is delivered by means of popular games.\nThat is why even the STEAM versions as well as the (free) Demos of such games are infected with it. SecuROM will borrow deep into our PC systems and will refuse to be removed completely even after uninstalling the game it came with. It will retain backdoor access and will keep reporting to its mothership.\n\nLately, these security concerns have been accentuated as known Trojans seem to be exploiting SecuROM's backdoor access for their own purposes. In effect, installing a SecuROM-infected game in our computer will be placing your hardware and data at risk long after having uninstalled the game.\n\nAnd the latest vehicle to deliver this hazardous snoopware is FAR CRY 2 - a game crippled by LIMITED INSTALLATIONS! No, thanks. I think I 'll pass this one too.\n\nThe only people who do not care about SecuROM are, in fact,...pirates! Because cracking games \"protected\" by this contraption apparently is very easy. Every single game that was supposedly \"protected\" by SecuROM was cracked hours withing its release!\nTo everyone else though, SecuROM (or StarFORCE or any other hazardous DRM scheme) is a core issue that needs to be resolved before PC gaming can evolve any further. And the best way to resolve such issues is market correction.\n\nThat is why it is important for gamers to keep voting with their wallets. And as with any vote, well informed decisions are paramount in making the right choice.\ncode didn't work, got me a refund.\nthese do not work at all, all i get is static and they came with nothing in the box to give any help.\nwell let me start by saying that when i first started this game up i was so excited, but when i started playing OMG was i pissed off. The graphics where that of a xbox original, i am not joking they are some of the WORST graphics i have seen in a long time. NOT only are the graphics bad but the game play is not much better. Besides the fact that when you walk you look like you have something rather large in your butt making your player look VERY stiff, and has for the story, WAIT there was a story here??? well if there was anything to do with a story then i must have missed it, the so called story here is not at ALL good in fact it was just pain boring and luckily for me was VERY short beating it in under 2 hours. Ok now on to the level design, well lets just say that it was some of the worst i have seen. It goes like this, run into to dark boring room shoot a few bad guys or zombies then move to the next room that looks almost the same then repeat and do again for the next few hours. Ok so now i will move to the sound of the game, so the zombies when you kill them it sounds OK at best, but as for the sound of the guns WOW OMFG the worst EVER sounds weaker then a nerf guns!!!! the voice overs are very bad as well. Ok now onto the only kind of fun in this game, the only time i had ANY fun on this game was when i played with friends, but even still it was very boring. There is a ranking system that is mediocre at best, the more enemy's you kill the more XP you will get, and when you earn XP you can buy guns that won't really care to use because they all for the most part suck, or you can buy powers for the different players you can choose from. I would also like to add in the fact that when i you enemy it is almost completely random as to when they die, sometimes it will only take a few shoots to kill a simple solder and other time the same kind of enemy will take a clip and a half to kill, as well as that but the hit indication is very OFF. So i will wrap this up by saying this, DO NOT BUY THIS GAME, rent it first to see for yourself. i hope someone can find fun in this game, but for me i could not find much fun here.\n\nI forgot to add in here that the AI IS THE WORST IN ANY GAME EVER. your team mates make no attempt whatsoever to avoid danger or even bother to shoot at the enemy, your friendly AI will spend the bulk of their time wasting ammo on a wall or ceiling if they are even shooting at ALL, the rest of the time they are just in your way block you in doorways or blocking your shots, never before have i ever seen AI worse than this. As for the enemy AI they are just as bad, half the time the enemy is just standing there as i am shooting them.\n\nSO this game is pretty much a FAIL on every level, its a broken game, But lets hope that resident evil 6 will make up for this REALLY REALLY BAD GAME\nDont waste your money, you will just end up using the nunchuck, or a classic controler in the end, and more over don't waste your money on the new Mario Cart, just keep, or get one of the older ones, its just recycled maps with new controls that aren't that good anyways.\n"
     ]
    }
   ],
   "source": [
    "print(*rating_sample)\n",
    "print(*review_sample, sep='\\n') # each review is seperated by a '-'"
   ]
  },
  {
   "source": [
    "### Word and sentence tokenization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant tokenization modules from nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       [recently, ubisoft, had, to, settle, a, huge, ...\n",
       "1        [code, did, n't, work, ,, got, me, a, refund, .]\n",
       "2       [these, do, not, work, at, all, ,, all, i, get...\n",
       "3       [well, let, me, start, by, saying, that, when,...\n",
       "4       [dont, waste, your, money, ,, you, will, just,...\n",
       "                              ...                        \n",
       "4495    [nice, long, micro, usb, cable, ,, battery, la...\n",
       "4496    [i, 've, been, having, a, great, time, with, t...\n",
       "4497                                                  [d]\n",
       "4498    [really, pretty, ,, funny, ,, interesting, gam...\n",
       "4499    [i, had, a, lot, of, fun, playing, this, game,...\n",
       "Name: reviews, Length: 4500, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# text normalization (in this ex phrases are lowercase) is a nice addition for text analysis\n",
    "# followed by the appropriate token parsing \n",
    "word_tokenization = df['reviews'].str.lower().apply(lambda x: word_tokenize(x))\n",
    "word_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       [recently ubisoft had to settle a huge class-a...\n",
       "1                    [code didn't work, got me a refund.]\n",
       "2       [these do not work at all, all i get is static...\n",
       "3       [well let me start by saying that when i first...\n",
       "4       [dont waste your money, you will just end up u...\n",
       "                              ...                        \n",
       "4495    [nice long micro usb cable, battery lasts a lo...\n",
       "4496    [i've been having a great time with this game....\n",
       "4497                                                  [d]\n",
       "4498    [really pretty, funny, interesting game., work...\n",
       "4499    [i had a lot of fun playing this game, if your...\n",
       "Name: reviews, Length: 4500, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "sent_tokenization = df['reviews'].str.lower().apply(lambda x: sent_tokenize(x))\n",
    "sent_tokenization"
   ]
  },
  {
   "source": [
    "### Download NLTK `opinion lexicon`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n[nltk_data]     /Users/ShuaibAhmed/nltk_data...\n[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# corresponding module import \n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine this module/corpus - i.e. first 10 \n",
    "positive = opinion_lexicon.positive()[:10]\n",
    "negative = opinion_lexicon.negative()[:10]\n",
    "words = sorted(opinion_lexicon.words())[:10] # sorted alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n['2-faced', '2-faces', 'a+', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort']\n"
     ]
    }
   ],
   "source": [
    "print(negative)\n",
    "print(positive)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2006\n4783\n6789\n"
     ]
    }
   ],
   "source": [
    "# check the length of each corpus/set \n",
    "print(len(opinion_lexicon.positive()))\n",
    "print(len(opinion_lexicon.negative()))\n",
    "print(len(opinion_lexicon.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to check/test if certain words are in the opinion_lexicon \n",
    "def word_check(word):\n",
    "    if word in opinion_lexicon.positive():\n",
    "        return f'{word} is positive'\n",
    "    elif word in opinion_lexicon.negative():\n",
    "        return f'{word} is negative' \n",
    "    else: \n",
    "        return f'{word} not covered in lexicon' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sad is negative\nbad is negative\nwonderful is positive\nAWESOME not covered in lexicon\nawesome is positive\n"
     ]
    }
   ],
   "source": [
    "print(word_check('sad'))\n",
    "print(word_check('bad'))\n",
    "print(word_check('wonderful'))\n",
    "# purposely checking that lexicon is always lowercase \n",
    "print(word_check('AWESOME')) \n",
    "print(word_check('awesome'))"
   ]
  },
  {
   "source": [
    "### Classify reviews - negative (-1) to positive (+1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "It is recommended to score the reviews in two steps: \n",
    "<br>\n",
    "1) First score the sentences of the reviews from Â–1 to 1 based on the sum of the positive and negative words they include. \n",
    "<br>\n",
    "2) Then count the sentiment score of the reviews, which you preliminary sliced into sentences."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef sentence_scoring(sents):\\n    word_selection = [w.lower() for w in sents if w.isalpha()]\\n    total_word_selection = len(word_selection)\\n    sum_positive = len([w for w in word_selection if w in opinion_lexicon.positive()])\\n    sum_negative = len([w for w in word_selection if w in opinion_lexicon.negative()])\\n    if total_word_selection > 0:\\n        return (sum_positive - sum_negative) / total_word_selection\\n    else:\\n        return 0\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "\"\"\"\n",
    "def sentence_scoring(sents):\n",
    "    word_selection = [w.lower() for w in sents if w.isalpha()]\n",
    "    total_word_selection = len(word_selection)\n",
    "    sum_positive = len([w for w in word_selection if w in opinion_lexicon.positive()])\n",
    "    sum_negative = len([w for w in word_selection if w in opinion_lexicon.negative()])\n",
    "    if total_word_selection > 0:\n",
    "        return (sum_positive - sum_negative) / total_word_selection\n",
    "    else:\n",
    "        return 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef review_score(review):\\n    sentiment_scores = list()\\n    sentences = sent_tokenize(review)\\n    for sent in sentences:\\n        words = word_tokenize(sent)\\n        sentence_score = sentence_scoring(words)\\n        sentiment_scores.append(sentence_score)\\n    if sentiment_scores:  # has at least 1 sentence score\\n        return sum(sentiment_scores) / len(sentiment_scores)\\n    else:  # return 0 if no sentiment_scores, avoid division by zero\\n        return 0\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "\"\"\"\n",
    "def review_score(review):\n",
    "    sentiment_scores = list()\n",
    "    sentences = sent_tokenize(review)\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        sentence_score = sentence_scoring(words)\n",
    "        sentiment_scores.append(sentence_score)\n",
    "    if sentiment_scores:  # has at least 1 sentence score\n",
    "        return sum(sentiment_scores) / len(sentiment_scores)\n",
    "    else:  # return 0 if no sentiment_scores, avoid division by zero\n",
    "        return 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence):\n",
    "    sentiment=0\n",
    "    words = [word.lower() for word in word_tokenize(sentence) if word.isalnum()]\n",
    "    for word in words:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            sentiment += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            sentiment -= 1\n",
    "        else:\n",
    "            sentiment = 0 \n",
    "    # normalize scores to make sure score is within -1/+1 range \n",
    "    return sentiment/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = df['reviews'].apply(sentiment)\n",
    "sentiment"
   ]
  }
 ]
}