{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding on document vectors, as aforementioned, words counts (basic or normalized by length of lexicon/document) don't tell us much about the importance of such word in the document *relative* to the rest of the documents in the corpus.\n",
    "<br>\n",
    "Hence solving a solution for this problem would mean we could start to describe documents within the corpus.\n",
    "<br>\n",
    "An example corpus such as every kite book written would generally mean the word 'Kite' will appear very frequently in every book (document) that we counted - which doesn't provide us with any useful information/data because it cannot differentiate/distinguish between those documents.\n",
    "<br>\n",
    "Some related words like 'aerodynamics' or 'wind' may not be common across the entire corpus, but for ones where it did frequently occur, we would know more about each document's nature. To accomplish this we need another tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency (IDF)** - Allows us to perform topic analysis corresponding to *Zipf's law*\n",
    "<br>\n",
    "A quick overview of such law seen from this [wiki](https://en.wikipedia.org/wiki/Zipf%27s_law)\n",
    "> Zipf's law states that given some corpus of natural language utterances, the frequency of any word is ***inversely proportional*** to its rank in the frequency table.\n",
    "* In summary if we rank the words of a corpus by the number of occurences and list them in descending order, for a decently large sample of documents, we'll find accordingly that the first word in the ranked list is twice as likely to occur as the second word in the list; it is also three times as likely to appear as the third word in the list\n",
    "* Given a large corpus, using this heuristic to illustrate statistically how likely a certain word is to appear in any certain document of that corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a term frequency counter, one can count tokens and bin them up in two ways\n",
    "<br>\n",
    "1) Per document\n",
    "<br>\n",
    "2) Across the entire corpus\n",
    "<br>\n",
    "<br>\n",
    "For now, we'll just focus on 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with the Kite corpora example - we'll retrieve the total word count for each document in our corpus (intro_doc and history_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "363"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text, kite_history # kite_intro and kite_hist respectively\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "intro_total = len(intro_tokens)\n",
    "intro_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "297"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "kite_hist = str(kite_history).lower()\n",
    "history_tokens = tokenizer.tokenize(kite_hist)\n",
    "history_total = len(history_tokens)\n",
    "history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we compiled a couple of tokenized kite documents at our disposal, let's look at the term frequency (TF) of 'kite' in each document \n",
    "<br>\n",
    "We'll store the TFs we find in two dictionaries - one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "intro_tf = {}\n",
    "history_tf = {} \n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite']/intro_total\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite']/history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'kite' in intro document is: 0.0441\nthe Term Frequency of 'kite' in history document is: 0.0202\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'kite' in intro document is: {intro_tf['kite']:.4f}\")\n",
    "print(f\"the Term Frequency of 'kite' in history document is: {history_tf['kite']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the printed statements, it looks to be that the TF proportion of the intro document is twice the size of the TF proportion of the history document. But we cannot say the intro portion is twice as much about kites.\n",
    "<br>\n",
    "Another thought experiment is to go a bit deeper and search for other related terms and the correspondin TF for them such as 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['and'] = intro_counts['and']/intro_total\n",
    "history_tf['and'] = history_counts['and']/history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'and' in intro document is: 0.0275\nthe Term Frequency of 'and' in history document is: 0.0303\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'and' in intro document is: {intro_tf['and']:.4f}\")\n",
    "print(f\"the Term Frequency of 'and' in history document is: {history_tf['and']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, both of these documents have something to say about 'and' just as much as 'kite'. But again this is not helpful for us as it is not revelatory given a quick view of both of these TFs. By this logic of the this tf within the document 'and' is seen as an important word in the document which is not the case - given our heuritstic of identifying stopwords/prepositions that should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to conceptualize a term's inverse document frequency (IDF) is to understand that if term appears in a document relatively frequently, but occurs rarely in the rest of the corpus, it's safe to assume that it's important to that document specifically. This is the basic foundation towards topic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term IDF** - Ratio of the total number documents to the number of documents the term appears in. It can be seen as a 'rarity' measure to weight the TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_and = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_kite = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_china = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "num_docs_containing_and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "# IDF ratio \n",
    "# Denom is 2 given that 'and' appears in 2 different documents in this corpus (intro_tokens and hist_tokens)\n",
    "len([intro_tokens, history_tokens]) / num_docs_containing_and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for China in the two documents \n",
    "intro_tf['china'] = intro_counts['china']/intro_total\n",
    "history_tf['china'] = history_counts['china']/history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's just a matter of acquiring the IDF for all three TFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2 \n",
    "intro_idf = {} \n",
    "history_idf = {} \n",
    "intro_idf['and'] = num_docs/num_docs_containing_and\n",
    "history_idf['and'] = num_docs/num_docs_containing_and\n",
    "intro_idf['kite'] = num_docs/num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs/num_docs_containing_kite\n",
    "intro_idf['china'] = num_docs/num_docs_containing_china\n",
    "history_idf['china'] = num_docs/num_docs_containing_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for intro document \n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'and': 0.027548209366391185, 'kite': 0.0440771349862259, 'china': 0.0}"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "intro_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'and': 0.030303030303030304,\n 'kite': 0.020202020202020204,\n 'china': 0.020202020202020204}"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "history_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an instance of 1 million documents as an example we can say the following\n",
    "* 'cat' - the term 'cat' appears in 1 document \n",
    "* 'dog' - the term 'dog' appears in 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The IDF of cat across 1 million documents is: 1,000,000\n"
    }
   ],
   "source": [
    "# cat IDF \n",
    "cat_idf = int(1_000_000/1)\n",
    "print(f'The IDF of cat across 1 million documents is: {cat_idf:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The IDF of dog across 1 million documents is: 100,000\n"
    }
   ],
   "source": [
    "# dog IDF \n",
    "dog_idf = int(1000000/10)\n",
    "print(f'The IDF of dog across 1 million documents is: {dog_idf:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, the diffence in size/scale appears large. Given Zipf's law, comparing frequencies between two words (even when the frequencies are relatively similar), the more frequent word will have an exponentially higher frequency than the less frequent one. \n",
    "<br>\n",
    "To control for this, Zipf's law also suggests that we scale all word/document frequencies using the `log()` function which is the inverse of the `exp()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # built-in library\n",
    "import numpy as np # scientific computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `log` technique ensures that words such as 'cat' and 'dog' - having relatively similar TF counts - aren't exponentially different in frequency. \n",
    "<br>\n",
    "Also, this distribution of word frequencies ensure that our TF-IDF scores are more uniformly distributed \n",
    "<br> \n",
    "The base of the log function is not important as we're only concerned about frequency distribution being uniform and not to scale it within a particular numerical range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6.0\n5.0\n"
    }
   ],
   "source": [
    "# Using base 10 log \n",
    "print(np.log10(cat_idf))\n",
    "print(np.log10(dog_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path().home()/'Desktop'/'nlp-map-project'/'chp3-nlpia-notes'/'img-vects'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/tfidf.png\" alt=\"tfidf formulation\" width=\"400\" height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 \n",
    "NLPIA Lane, Howard and Hapke (2019) chp 3.4.1 pp. 254 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 summarises the tfidf formulae \n",
    "* t - a given term \n",
    "* d - a given document \n",
    "* D - A given corpus (collection of documents in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Heuristic:***\n",
    "* TF - The more times a word appears in the document (corresponding with TF-IDF overall), the TF also increases\n",
    "* IDF - As the number of documents that contain that word goes up, the IDF (corresponding with the TF-IDF overall) for that word will decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in again the 'Harry' corpus example \n",
    "from nlpia.data.loaders import harry_docs as docs\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "33\n18\n[',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy', 'harry', 'home', 'is', 'jill', 'not', 'store', 'than', 'the', 'to', 'would']\n"
    }
   ],
   "source": [
    "doc_tokens = [] \n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))\n",
    "\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "print(len(all_doc_tokens))\n",
    "\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "\n",
    "print(len(lexicon))\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([(',', 0),\n             ('.', 0),\n             ('and', 0),\n             ('as', 0),\n             ('faster', 0),\n             ('get', 0),\n             ('got', 0),\n             ('hairy', 0),\n             ('harry', 0),\n             ('home', 0),\n             ('is', 0),\n             ('jill', 0),\n             ('not', 0),\n             ('store', 0),\n             ('than', 0),\n             ('the', 0),\n             ('to', 0),\n             ('would', 0)])"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    # copy ensures that we create a separate object to reference and not overwriting as we       iterate \n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0 \n",
    "        for doc_ in docs:\n",
    "            if key in doc_:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs)/docs_containing_key\n",
    "        else:\n",
    "            idf = 0 \n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(document_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "          ,         .       and        as    faster       get       got  \\\n0  0.166667  0.055556  0.083333  0.000000  0.250000  0.166667  0.166667   \n1  0.000000  0.055556  0.083333  0.000000  0.083333  0.000000  0.000000   \n2  0.000000  0.055556  0.000000  0.111111  0.000000  0.000000  0.000000   \n\n      hairy  harry      home        is  jill       not     store      than  \\\n0  0.000000    0.0  0.166667  0.000000   0.0  0.000000  0.166667  0.000000   \n1  0.083333    0.0  0.000000  0.083333   0.0  0.000000  0.000000  0.166667   \n2  0.083333    0.0  0.000000  0.083333   0.0  0.166667  0.000000  0.000000   \n\n   the        to     would  \n0  0.5  0.166667  0.166667  \n1  0.0  0.000000  0.000000  \n2  0.0  0.000000  0.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>,</th>\n      <th>.</th>\n      <th>and</th>\n      <th>as</th>\n      <th>faster</th>\n      <th>get</th>\n      <th>got</th>\n      <th>hairy</th>\n      <th>harry</th>\n      <th>home</th>\n      <th>is</th>\n      <th>jill</th>\n      <th>not</th>\n      <th>store</th>\n      <th>than</th>\n      <th>the</th>\n      <th>to</th>\n      <th>would</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.166667</td>\n      <td>0.055556</td>\n      <td>0.083333</td>\n      <td>0.000000</td>\n      <td>0.250000</td>\n      <td>0.166667</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.5</td>\n      <td>0.166667</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.055556</td>\n      <td>0.083333</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.055556</td>\n      <td>0.000000</td>\n      <td>0.111111</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.083333</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "# Store the document tfidf vectors in a dataframe for better visibility\n",
    "# rows denote the different sentences/documents with tfidf word vectors (lexicon) as features\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This TFIDF vector representation is clever in considering that the term which is the main topic ('Harry') is given a (low) weight of 0 as it understands that such TF appears in every doc and is relatively frequent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we have a K-dimensional vector representation of each document in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the DataFrame dimensions (row x columns) for the tfidf word vectors: (3, 18)\n"
    }
   ],
   "source": [
    "print(f'the DataFrame dimensions (row x columns) for the tfidf word vectors: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand on this transformation and use for cases pertaining to search - where two vectors in a given vector space are shown to be similar if they have a similar/close angle.\n",
    "<br>\n",
    "As a reminder, we use cosine similarity to measure such similarity which is likely to be similar when the cosine similarity score is high.\n",
    "<br>\n",
    "The formulae to minimise (optimise) is:\n",
    "<br>\n",
    "$cos (\\Theta$) = $\\frac{A \\times B}{|A| \\times |B|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a simple TF-IDF search, treat the search query itself as a document and therefore get the TF-IDF based vector representation of it.\n",
    "<br>\n",
    "Then find the documents whose vectors have the highest cosine similarities to the query and return such computations as search results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again take example of the three documents about Harry and use the example search query:\n",
    "> “How long does it take to get to the store?”\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How long does it take to get to the store?'\n",
    "query_vec = copy.copy(zero_vector)\n",
    "query_vec = copy.copy(zero_vector)\n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0 \n",
    "    for doc_ in docs:\n",
    "        if key in doc_.lower():\n",
    "            docs_containing_key += 1\n",
    "        if docs_containing_key == 0:\n",
    "    # continue is used here to skip to the next key if such token isn't found in the lexicon\n",
    "    # This is also used to avoid the 'division' by zero error \n",
    "            continue \n",
    "        tf = value/len(tokens)\n",
    "        idf = len(docs)/docs_containing_key\n",
    "        query_vec[key] = tf * idf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vect_a, vect_b):\n",
    "    \"\"\" Convert from dictionary to lists for easier computation and matching \"\"\"\n",
    "    vect_a = [val for val in vect_a.values()]\n",
    "    vect_b = [val for val in vect_b.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vect_a):\n",
    "        dot_prod += v * vect_b[i]\n",
    "    \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vect_a]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vect_b]))\n",
    "\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cosine similarity between search query and first document in Harry corpus: 0.6132857433407973\n"
    }
   ],
   "source": [
    "print(f'cosine similarity between search query and first document in Harry corpus: {cos_sim(query_vec, document_tfidf_vectors[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cosine similarity between search query and second document in Harry corpus: 0.0\n"
    }
   ],
   "source": [
    "print(f'cosine similarity between search query and second document in Harry corpus: {cos_sim(query_vec, document_tfidf_vectors[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cosine similarity between search query and third document in Harry corpus: 0.0\n"
    }
   ],
   "source": [
    "print(f'cosine similarity between search query and third document in Harry corpus: {cos_sim(query_vec, document_tfidf_vectors[2])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that that first document (doc 0) contains the most relevance for our search query.\n",
    "* This simple technique of TFIDF representation and taking the cosine similarities enables the application of finding relevant documents in any corpus using keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major search engines like Google *inverted index* that is somewhat safe from competition as it has a time complexity of O(1) - constant time which is the fastest possible algorithm run time - as opposed to our simple TF-IDF vector search that uses an 'index scan' for each query consisting of O(N) - linear time which *slightly* slower then constant time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B - To make the above search querying similarities comparison more effective, rather than ignoring/dropping the keys that weren't found in the lexicon to avoid the denominator being 0 (i.e. division by zero error), a better approach is to add +1 to the denominator of every IDF calculation and avoding such erroneous situations.\n",
    "<br>\n",
    "This approach is known as the ***additive smoothing (laplace smoothing)***. When compiled, it'll usually improve the search results for TF-IDF keyword-based searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools (automating the search similarity pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools/packages have been designed in this domain to mostly automate the above standard python built-in code and reducing the number of lines/work needed for the same computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs \n",
    "vectorizer = TfidfVectorizer(min_df=1) # default min_df is 1 anyway\n",
    "model = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<bound method spmatrix.get_shape of <3x16 sparse matrix of type '<class 'numpy.float64'>'\n\twith 23 stored elements in Compressed Sparse Row format>>\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(3, 16)"
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "# as above when using TF-IDF vector representation, we have here 3 documents with 16 word vector features (the model here already ignores the two puncuation tokens for us)\n",
    "print(model.get_shape) # sparse matrix \n",
    "model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "matrix([[0.1614879 , 0.        , 0.48446369, 0.21233718, 0.21233718,\n         0.        , 0.25081952, 0.21233718, 0.        , 0.        ,\n         0.        , 0.21233718, 0.        , 0.63701154, 0.21233718,\n         0.21233718],\n        [0.36930805, 0.        , 0.36930805, 0.        , 0.        ,\n         0.36930805, 0.28680065, 0.        , 0.36930805, 0.36930805,\n         0.        , 0.        , 0.48559571, 0.        , 0.        ,\n         0.        ],\n        [0.        , 0.75143242, 0.        , 0.        , 0.        ,\n         0.28574186, 0.22190405, 0.        , 0.28574186, 0.28574186,\n         0.37571621, 0.        , 0.        , 0.        , 0.        ,\n         0.        ]])"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "model.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we get the matrix representation of the TF-IDF computations for each word vector - like a list of lists.\n",
    "<br>\n",
    "We can be even more clearer and add the corresponding vocabulary for this vector representation where each column is the word vector and the row represents each document using a **DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    and    as  faster   get   got  hairy  harry  home    is  jill   not  \\\n0  0.16  0.00    0.48  0.21  0.21   0.00   0.25  0.21  0.00  0.00  0.00   \n1  0.37  0.00    0.37  0.00  0.00   0.37   0.29  0.00  0.37  0.37  0.00   \n2  0.00  0.75    0.00  0.00  0.00   0.29   0.22  0.00  0.29  0.29  0.38   \n\n   store  than   the    to  would  \n0   0.21  0.00  0.64  0.21   0.21  \n1   0.00  0.49  0.00  0.00   0.00  \n2   0.00  0.00  0.00  0.00   0.00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>and</th>\n      <th>as</th>\n      <th>faster</th>\n      <th>get</th>\n      <th>got</th>\n      <th>hairy</th>\n      <th>harry</th>\n      <th>home</th>\n      <th>is</th>\n      <th>jill</th>\n      <th>not</th>\n      <th>store</th>\n      <th>than</th>\n      <th>the</th>\n      <th>to</th>\n      <th>would</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.16</td>\n      <td>0.00</td>\n      <td>0.48</td>\n      <td>0.21</td>\n      <td>0.21</td>\n      <td>0.00</td>\n      <td>0.25</td>\n      <td>0.21</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.21</td>\n      <td>0.00</td>\n      <td>0.64</td>\n      <td>0.21</td>\n      <td>0.21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.37</td>\n      <td>0.29</td>\n      <td>0.00</td>\n      <td>0.37</td>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>0.75</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.29</td>\n      <td>0.22</td>\n      <td>0.00</td>\n      <td>0.29</td>\n      <td>0.29</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "df = pd.DataFrame(model.todense().round(2), columns= vectorizer.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also the use the `scikit-learn` library and import the function `cosine_similarity` to return a cosine_similarity for each query we want to search for as aforementioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_query_similarity(vectorizer, docs_tfidf, query):\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.56179137, 0.        , 0.        ])"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "get_tf_idf_query_similarity(vectorizer, model, 'How long does it take to get to the store?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above the first document in the Harry corpus is the most relevant to our search query - where the cosine similarity numbers are very similar to the cosine similarity measures we computed above (strictly python code).\n",
    "<br>\n",
    "Overall, when we use large texts, these pre-optimised TF-IDF model will save loads of work and time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}