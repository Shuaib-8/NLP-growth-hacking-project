{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding on document vectors, as aforementioned, words counts (basic or normalized by length of lexicon/document) don't tell us much about the importance of such word in the document *relative* to the rest of the documents in the corpus.\n",
    "<br>\n",
    "Hence solving a solution for this problem would mean we could start to describe documents within the corpus.\n",
    "<br>\n",
    "An example corpus such as every kite book written would generally mean the word 'Kite' will appear very frequently in every book (document) that we counted - which doesn't provide us with any useful information/data because it cannot differentiate/distinguish between those documents.\n",
    "<br>\n",
    "Some related words like 'aerodynamics' or 'wind' may not be common across the entire corpus, but for ones where it did frequently occur, we would know more about each document's nature. To accomplish this we need another tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency (IDF)** - Allows us to perform topic analysis corresponding to *Zipf's law*\n",
    "<br>\n",
    "A quick overview of such law seen from this [wiki](https://en.wikipedia.org/wiki/Zipf%27s_law)\n",
    "> Zipf's law states that given some corpus of natural language utterances, the frequency of any word is ***inversely proportional*** to its rank in the frequency table.\n",
    "* In summary if we rank the words of a corpus by the number of occurences and list them in descending order, for a decently large sample of documents, we'll find accordingly that the first word in the ranked list is twice as likely to occur as the second word in the list; it is also three times as likely to appear as the third word in the list\n",
    "* Given a large corpus, using this heuristic to illustrate statistically how likely a certain word is to appear in any certain document of that corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a term frequency counter, one can count tokens and bin them up in two ways\n",
    "<br>\n",
    "1) Per document\n",
    "<br>\n",
    "2) Across the entire corpus\n",
    "<br>\n",
    "<br>\n",
    "For now, we'll just focus on 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with the Kite corpora example - we'll retrieve the total word count for each document in our corpus (intro_doc and history_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "363"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text, kite_history # kite_intro and kite_hist respectively\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "intro_total = len(intro_tokens)\n",
    "intro_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "297"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "kite_hist = str(kite_history).lower()\n",
    "hist_tokens = tokenizer.tokenize(kite_hist)\n",
    "hist_total = len(hist_tokens)\n",
    "hist_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we compiled a couple of tokenized kite documents at our disposal, let's look at the term frequency (TF) of 'kite' in each document \n",
    "<br>\n",
    "We'll store the TFs we find in two dictionaries - one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "intro_tf = {}\n",
    "history_tf = {} \n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite']/intro_total\n",
    "hist_counts = Counter(hist_tokens)\n",
    "history_tf['kite'] = hist_counts['kite']/hist_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'kite' in intro document is: 0.0441\nthe Term Frequency of 'kite' in history document is: 0.0202\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'kite' in intro document is: {intro_tf['kite']:.4f}\")\n",
    "print(f\"the Term Frequency of 'kite' in history document is: {history_tf['kite']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the printed statements, it looks to be that the TF proportion of the intro document is twice the size of the TF proportion of the history document. But we cannot say the intro portion is twice as much about kites.\n",
    "<br>\n",
    "Another thought experiment is to go a bit deeper and search for other related terms and the correspondin TF for them such as 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['and'] = intro_counts['and']/intro_total\n",
    "history_tf['and'] = hist_counts['and']/hist_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'and' in intro document is: 0.0275\nthe Term Frequency of 'and' in history document is: 0.0303\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'and' in intro document is: {intro_tf['and']:.4f}\")\n",
    "print(f\"the Term Frequency of 'and' in history document is: {history_tf['and']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, both of these documents have something to say about 'and' just as much as 'kite'. But again this is not helpful for us as it is not revelatory given a quick view of both of these TFs. By this logic of the this tf within the document 'and' is seen as an important word in the document which is not the case - given our heuritstic of identifying stopwords/prepositions that should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to conceptualize a term's inverse document frequency (IDF) is to understand that if term appears in a document relatively frequently, but occurs rarely in the rest of the corpus, it's safe to assume that it's important to that document specifically. This is the basic foundation towards topic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term IDF** - Ratio of the total number documents to the number of documents the term appears in. It can be seen as a 'rarity' measure to weight the TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_and = 0 \n",
    "for doc in [intro_tokens, hist_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "num_docs_containing_and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# IDF ratio \n",
    "# Denom is 2 given there are two docs in this corpus (intro_tokens and hist_tokens)\n",
    "num_docs_containing_and / len([intro_tokens, hist_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}