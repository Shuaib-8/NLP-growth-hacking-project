{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding on document vectors, as aforementioned, words counts (basic or normalized by length of lexicon/document) don't tell us much about the importance of such word in the document *relative* to the rest of the documents in the corpus.\n",
    "<br>\n",
    "Hence solving a solution for this problem would mean we could start to describe documents within the corpus.\n",
    "<br>\n",
    "An example corpus such as every kite book written would generally mean the word 'Kite' will appear very frequently in every book (document) that we counted - which doesn't provide us with any useful information/data because it cannot differentiate/distinguish between those documents.\n",
    "<br>\n",
    "Some related words like 'aerodynamics' or 'wind' may not be common across the entire corpus, but for ones where it did frequently occur, we would know more about each document's nature. To accomplish this we need another tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inverse Document Frequency (IDF)** - Allows us to perform topic analysis corresponding to *Zipf's law*\n",
    "<br>\n",
    "A quick overview of such law seen from this [wiki](https://en.wikipedia.org/wiki/Zipf%27s_law)\n",
    "> Zipf's law states that given some corpus of natural language utterances, the frequency of any word is ***inversely proportional*** to its rank in the frequency table.\n",
    "* In summary if we rank the words of a corpus by the number of occurences and list them in descending order, for a decently large sample of documents, we'll find accordingly that the first word in the ranked list is twice as likely to occur as the second word in the list; it is also three times as likely to appear as the third word in the list\n",
    "* Given a large corpus, using this heuristic to illustrate statistically how likely a certain word is to appear in any certain document of that corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a term frequency counter, one can count tokens and bin them up in two ways\n",
    "<br>\n",
    "1) Per document\n",
    "<br>\n",
    "2) Across the entire corpus\n",
    "<br>\n",
    "<br>\n",
    "For now, we'll just focus on 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with the Kite corpora example - we'll retrieve the total word count for each document in our corpus (intro_doc and history_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "363"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text, kite_history # kite_intro and kite_hist respectively\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "intro_total = len(intro_tokens)\n",
    "intro_total "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "297"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "kite_hist = str(kite_history).lower()\n",
    "history_tokens = tokenizer.tokenize(kite_hist)\n",
    "history_total = len(history_tokens)\n",
    "history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given we compiled a couple of tokenized kite documents at our disposal, let's look at the term frequency (TF) of 'kite' in each document \n",
    "<br>\n",
    "We'll store the TFs we find in two dictionaries - one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "intro_tf = {}\n",
    "history_tf = {} \n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite']/intro_total\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite']/history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'kite' in intro document is: 0.0441\nthe Term Frequency of 'kite' in history document is: 0.0202\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'kite' in intro document is: {intro_tf['kite']:.4f}\")\n",
    "print(f\"the Term Frequency of 'kite' in history document is: {history_tf['kite']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the printed statements, it looks to be that the TF proportion of the intro document is twice the size of the TF proportion of the history document. But we cannot say the intro portion is twice as much about kites.\n",
    "<br>\n",
    "Another thought experiment is to go a bit deeper and search for other related terms and the correspondin TF for them such as 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf['and'] = intro_counts['and']/intro_total\n",
    "history_tf['and'] = history_counts['and']/history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "the Term Frequency of 'and' in intro document is: 0.0275\nthe Term Frequency of 'and' in history document is: 0.0303\n"
    }
   ],
   "source": [
    "print(f\"the Term Frequency of 'and' in intro document is: {intro_tf['and']:.4f}\")\n",
    "print(f\"the Term Frequency of 'and' in history document is: {history_tf['and']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, both of these documents have something to say about 'and' just as much as 'kite'. But again this is not helpful for us as it is not revelatory given a quick view of both of these TFs. By this logic of the this tf within the document 'and' is seen as an important word in the document which is not the case - given our heuritstic of identifying stopwords/prepositions that should be filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to conceptualize a term's inverse document frequency (IDF) is to understand that if term appears in a document relatively frequently, but occurs rarely in the rest of the corpus, it's safe to assume that it's important to that document specifically. This is the basic foundation towards topic analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term IDF** - Ratio of the total number documents to the number of documents the term appears in. It can be seen as a 'rarity' measure to weight the TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_and = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_kite = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs_containing_china = 0 \n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "num_docs_containing_and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# IDF ratio \n",
    "# Denom is 2 given that 'and' appears in 2 different documents in this corpus (intro_tokens and hist_tokens)\n",
    "len([intro_tokens, history_tokens]) / num_docs_containing_and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF for China in the two documents \n",
    "intro_tf['china'] = intro_counts['china']/intro_total\n",
    "history_tf['china'] = history_counts['china']/history_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's just a matter of acquiring the IDF for all three TFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 2 \n",
    "intro_idf = {} \n",
    "history_idf = {} \n",
    "intro_idf['and'] = num_docs/num_docs_containing_and\n",
    "history_idf['and'] = num_docs/num_docs_containing_and\n",
    "intro_idf['kite'] = num_docs/num_docs_containing_kite\n",
    "history_idf['kite'] = num_docs/num_docs_containing_kite\n",
    "intro_idf['china'] = num_docs/num_docs_containing_china\n",
    "history_idf['china'] = num_docs/num_docs_containing_china"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for intro document \n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'and': 0.027548209366391185, 'kite': 0.0440771349862259, 'china': 0.0}"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "intro_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * hist_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'and': 0.030303030303030304,\n 'kite': 0.020202020202020204,\n 'china': 0.020202020202020204}"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "history_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf application "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an instance of 1 million documents as an example we can say the following\n",
    "* 'cat' - the term 'cat' appears in 1 document \n",
    "* 'dog' - the term 'dog' appears in 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The IDF of cat across 1 million documents is: 1,000,000\n"
    }
   ],
   "source": [
    "# cat IDF \n",
    "cat_idf = int(1_000_000/1)\n",
    "print(f'The IDF of cat across 1 million documents is: {cat_idf:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The IDF of dog across 1 million documents is: 100,000\n"
    }
   ],
   "source": [
    "# dog IDF \n",
    "dog_idf = int(1000000/10)\n",
    "print(f'The IDF of dog across 1 million documents is: {dog_idf:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, the diffence in size/scale appears large. Given Zipf's law, comparing frequencies between two words (even when the frequencies are relatively similar), the more frequent word will have an exponentially higher frequency than the less frequent one. \n",
    "<br>\n",
    "To control for this, Zipf's law also suggests that we scale all word/document frequencies using the `log()` function which is the inverse of the `exp()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # built-in library\n",
    "import numpy as np # scientific computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this `log` technique ensures that words such as 'cat' and 'dog' - having relatively similar TF counts - aren't exponentially different in frequency. \n",
    "<br>\n",
    "Also, this distribution of word frequencies ensure that our TF-IDF scores are more uniformly distributed \n",
    "<br> \n",
    "The base of the log function is not important as we're only concerned about frequency distribution being uniform and not to scale it within a particular numerical range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6.0\n5.0\n"
    }
   ],
   "source": [
    "# Using base 10 log \n",
    "print(np.log10(cat_idf))\n",
    "print(np.log10(dog_idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path().home()/'Desktop'/'nlp-map-project'/'chp3-nlpia-notes'/'img-vects'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/tfidf.png\" alt=\"tfidf formulation\" width=\"400\" height='250'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 \n",
    "NLPIA Lane, Howard and Hapke (2019) chp 3.4.1 pp. 254 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 summarises the tfidf formulae \n",
    "* t - a given term \n",
    "* d - a given document \n",
    "* D - A given corpus (collection of documents in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Heuristic:***\n",
    "* TF - The more times a word appears in the document (corresponding with TF-IDF overall), the TF also increases\n",
    "* IDF - As the number of documents that contain that word goes up, the IDF (corresponding with the TF-IDF overall) for that word will decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}