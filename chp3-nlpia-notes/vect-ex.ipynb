{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple foundation, the **BOW** representation allowed text to be represented in a mathematical form in some way that represents describing a document in terms of a frequency dictionary.\n",
    "<br>\n",
    "The next step is to go further and represent such textual data into a **vector** of those word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from collections import Counter \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    doc_vector.append(value/doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.07207207207207207,\n 0.06756756756756757,\n 0.036036036036036036,\n 0.02252252252252252,\n 0.018018018018018018]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Retrieve the first five most common tokens in this vector\n",
    "doc_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note - as these document vectors get larger, it's best to deviate away from python built-ins and exploit data structures that inherently utilise vectorization such as `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using vectors and applying mathematical operations on them relies on them being relative to a common feature across all such vectors.\n",
    "<br>\n",
    "The mathematical operations on means that vectors need to represent a position in common space - relative to something consistent.\n",
    "* Vector considerations - Vectors need to have the same origin and share the same scale (also units) on each of their dimensions\n",
    "<br>\n",
    "<br>\n",
    "1) The first step is to normalize the counts by calculating normalized term frequency instead of raw count(s) in the document \n",
    "<br>\n",
    "2) The second step is to ensure that all the vectors are in the form of standard length or dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want the value for each element of the vector to represent the same word in each document's vector.\n",
    "* ***lexicon*** - The collection of (distinct) words in the vocabulary comes in this case where we find every unique word in the union of such multiple sets (combination of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The faster Harry got to the store, the faster and faster Harry would get home.',\n 'Harry is hairy and faster than Jill.',\n 'Jill is not as hairy as Harry.']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from nlpia.data.loaders import harry_docs as docs\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = [] \n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "33"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[',',\n '.',\n 'and',\n 'as',\n 'faster',\n 'get',\n 'got',\n 'hairy',\n 'harry',\n 'home',\n 'is',\n 'jill',\n 'not',\n 'store',\n 'than',\n 'the',\n 'to',\n 'would']"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(lexicon)\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each of three document vectors would need to exhibit 18 values - even if a certain document for its corresponding vector doesn't contain all 18 words in our lexicon.\n",
    "* Each token is assigned a *slot* in the vectors corresponding to its position in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([(',', 0),\n             ('.', 0),\n             ('and', 0),\n             ('as', 0),\n             ('faster', 0),\n             ('get', 0),\n             ('got', 0),\n             ('hairy', 0),\n             ('harry', 0),\n             ('home', 0),\n             ('is', 0),\n             ('jill', 0),\n             ('not', 0),\n             ('store', 0),\n             ('than', 0),\n             ('the', 0),\n             ('to', 0),\n             ('would', 0)])"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Use this adv data structure to remember original order of lexicon (based on insertion)\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "doc_vectors = [] \n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value/len(lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[OrderedDict([(',', 0.05555555555555555),\n              ('.', 0.05555555555555555),\n              ('and', 0.05555555555555555),\n              ('as', 0),\n              ('faster', 0.16666666666666666),\n              ('get', 0.05555555555555555),\n              ('got', 0.05555555555555555),\n              ('hairy', 0),\n              ('harry', 0.1111111111111111),\n              ('home', 0.05555555555555555),\n              ('is', 0),\n              ('jill', 0),\n              ('not', 0),\n              ('store', 0.05555555555555555),\n              ('than', 0),\n              ('the', 0.16666666666666666),\n              ('to', 0.05555555555555555),\n              ('would', 0.05555555555555555)]),\n OrderedDict([(',', 0),\n              ('.', 0.05555555555555555),\n              ('and', 0.05555555555555555),\n              ('as', 0),\n              ('faster', 0.05555555555555555),\n              ('get', 0),\n              ('got', 0),\n              ('hairy', 0.05555555555555555),\n              ('harry', 0.05555555555555555),\n              ('home', 0),\n              ('is', 0.05555555555555555),\n              ('jill', 0.05555555555555555),\n              ('not', 0),\n              ('store', 0),\n              ('than', 0.05555555555555555),\n              ('the', 0),\n              ('to', 0),\n              ('would', 0)]),\n OrderedDict([(',', 0),\n              ('.', 0.05555555555555555),\n              ('and', 0),\n              ('as', 0.1111111111111111),\n              ('faster', 0),\n              ('get', 0),\n              ('got', 0),\n              ('hairy', 0.05555555555555555),\n              ('harry', 0.05555555555555555),\n              ('home', 0),\n              ('is', 0.05555555555555555),\n              ('jill', 0.05555555555555555),\n              ('not', 0.05555555555555555),\n              ('store', 0),\n              ('than', 0),\n              ('the', 0),\n              ('to', 0),\n              ('would', 0)])]"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a coarse way to make word (count) vectors for each document in python using built-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path().home()/'Desktop'/'nlp-map-project'/'chp3-nlpia-notes'/'img-vects'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/NLPIA-vect-2D.png\" alt=\"Vectors in 2D space\" width=\"400\" height='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1\n",
    "<br>\n",
    "*NLPIA* Lane, Howard and Hapke (2019) chp 3.2.1 pp. 234 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are the foundation of Linear Algebra and such associated computations\n",
    "* *Space* - collection of all possible vectors that could appear in such space representation\n",
    "* Representation - ordered list of numbers/coordinates in a vector space, that are *rectilinear/Euclidean*\n",
    "* Association - they illustrate a location or position in that space or they are used to represent direction and magnitude/distance\n",
    "* Dimensions - A 2D space as in Figure 1 shows vectors with two values, where similar logic for a 3D space would mean vectors with three values etc.\n",
    "<br>\n",
    "<br>\n",
    "Bear in mind that some vectors with 2D values ***cannot*** be represented in a normal 2D space as above such as Geospatial coordinates representing longitude and latitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding a natural language document vector space, the ***dimensionality*** of our vector space is the **count of the number of distinct words that appear in the whole corpus**.\n",
    "<br>\n",
    "Notation for dimensionality can be represented as the following: \n",
    "* TF - The dimensionality can be represented as **\"K\"**\n",
    "* Distinct words/lexicon - Represented as the vocabulary size of the corpus, academic papers denote this with **\"|V|\"**\n",
    "<br>\n",
    "It's easy from then on to describe each document within such **K-dimensional vector space** by a **K-dimensional vector** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our simple example of `doc_vectors`, K = 18 in our three document corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/NLPIA-tf-freq-angle-vects.png\" alt=\"term frequency word vectors and corresponding angles in 2D space\" width=\"400\" height='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2\n",
    "<br>\n",
    "*NLPIA* Lane, Howard and Hapke (2019) chp 3.2.1 pp. 238 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the example more easy to visualize, as seen in figure 2, **K** is reduced to two (given 2D view) from the 18 dimensional vector space constructed from our original lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity** - Two vectors are similar when they share similar direction\n",
    "<br>\n",
    "**Magnitude** - represents the \"length\" of the vector, which would mean that two vectors having similar length translates to word count (TF) vectors possessing  the same length\n",
    "<br>\n",
    "<br>\n",
    "The estimate of document similarity provides us with searching the use of the same words about the same number of times in similar proportions.\n",
    "<br>\n",
    "Such metric would provide us with being familiar of the documents they represent as being more likely talking about similar things/topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cos (\\Theta$) = $(A * B)/$($|A| * |B|)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}