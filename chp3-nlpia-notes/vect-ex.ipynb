{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple foundation, the **BOW** representation allowed text to be represented in a mathematical form in some way that represents describing a document in terms of a frequency dictionary.\n",
    "<br>\n",
    "The next step is to go further and represent such textual data into a **vector** of those word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from collections import Counter \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    doc_vector.append(value/doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.07207207207207207,\n 0.06756756756756757,\n 0.036036036036036036,\n 0.02252252252252252,\n 0.018018018018018018]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Retrieve the first five most common tokens in this vector\n",
    "doc_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note - as these document vectors get larger, it's best to deviate away from python built-ins and exploit data structures that inherently utilise vectorization such as `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using vectors and applying mathematical operations on them relies on them being relative to a common feature across all such vectors.\n",
    "<br>\n",
    "The mathematical operations on means that vectors need to represent a position in common space - relative to something consistent.\n",
    "* Vector considerations - Vectors need to have the same origin and share the same scale (also units) on each of their dimensions\n",
    "<br>\n",
    "<br>\n",
    "1) The first step is to normalize the counts by calculating normalized term frequency instead of raw count(s) in the document \n",
    "<br>\n",
    "2) The second step is to ensure that all the vectors are in the form of standard length or dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want the value for each element of the vector to represent the same word in each document's vector.\n",
    "* ***lexicon*** - The collection of (distinct) words in the vocabulary comes in this case where we find every unique word in the union of such multiple sets (combination of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The faster Harry got to the store, the faster and faster Harry would get home.',\n 'Harry is hairy and faster than Jill.',\n 'Jill is not as hairy as Harry.']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from nlpia.data.loaders import harry_docs as docs\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = [] \n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "33"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[',',\n '.',\n 'and',\n 'as',\n 'faster',\n 'get',\n 'got',\n 'hairy',\n 'harry',\n 'home',\n 'is',\n 'jill',\n 'not',\n 'store',\n 'than',\n 'the',\n 'to',\n 'would']"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(lexicon)\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each of three document vectors would need to exhibit 18 values - even if a certain document for a its corresponding vector doesn't contain all 18 words in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}