{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple foundation, the **BOW** representation allowed text to be represented in a mathematical form in some way that represents describing a document in terms of a frequency dictionary.\n",
    "<br>\n",
    "The next step is to go further and represent such textual data into a **vector** of those word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import nltk\n",
    "from collections import Counter \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    doc_vector.append(value/doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.07207207207207207,\n 0.06756756756756757,\n 0.036036036036036036,\n 0.02252252252252252,\n 0.018018018018018018]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Retrieve the first five most common tokens in this vector\n",
    "doc_vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical note - as these document vectors get larger, it's best to deviate away from python built-ins and exploit data structures that inherently utilise vectorization such as `numpy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of using vectors and applying mathematical operations on them relies on them being relative to a common feature across all such vectors.\n",
    "<br>\n",
    "The mathematical operations on means that vectors need to represent a position in common space - relative to something consistent.\n",
    "* Vector considerations - Vectors need to have the same origin and share the same scale (also units) on each of their dimensions\n",
    "<br>\n",
    "<br>\n",
    "1) The first step is to normalize the counts by calculating normalized term frequency instead of raw count(s) in the document \n",
    "<br>\n",
    "2) The second step is to ensure that all the vectors are in the form of standard length or dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want the value for each element of the vector to represent the same word in each document's vector.\n",
    "* ***lexicon*** - The collection of (distinct) words in the vocabulary comes in this case where we find every unique word in the union of such multiple sets (combination of documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The faster Harry got to the store, the faster and faster Harry would get home.',\n 'Harry is hairy and faster than Jill.',\n 'Jill is not as hairy as Harry.']"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from nlpia.data.loaders import harry_docs as docs\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = [] \n",
    "for doc in docs:\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "33"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "18\n[',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy', 'harry', 'home', 'is', 'jill', 'not', 'store', 'than', 'the', 'to', 'would']\n"
    }
   ],
   "source": [
    "print(len(lexicon))\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, each of three document vectors would need to exhibit 18 values - even if a certain document for its corresponding vector doesn't contain all 18 words in our lexicon.\n",
    "* Each token is assigned a *slot* in the vectors corresponding to its position in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([(',', 0),\n             ('.', 0),\n             ('and', 0),\n             ('as', 0),\n             ('faster', 0),\n             ('get', 0),\n             ('got', 0),\n             ('hairy', 0),\n             ('harry', 0),\n             ('home', 0),\n             ('is', 0),\n             ('jill', 0),\n             ('not', 0),\n             ('store', 0),\n             ('than', 0),\n             ('the', 0),\n             ('to', 0),\n             ('would', 0)])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Use this adv data structure to remember original order of lexicon (based on insertion)\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "doc_vectors = [] \n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value/len(lexicon)\n",
    "    doc_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[OrderedDict([(',', 0.05555555555555555),\n              ('.', 0.05555555555555555),\n              ('and', 0.05555555555555555),\n              ('as', 0),\n              ('faster', 0.16666666666666666),\n              ('get', 0.05555555555555555),\n              ('got', 0.05555555555555555),\n              ('hairy', 0),\n              ('harry', 0.1111111111111111),\n              ('home', 0.05555555555555555),\n              ('is', 0),\n              ('jill', 0),\n              ('not', 0),\n              ('store', 0.05555555555555555),\n              ('than', 0),\n              ('the', 0.16666666666666666),\n              ('to', 0.05555555555555555),\n              ('would', 0.05555555555555555)]),\n OrderedDict([(',', 0),\n              ('.', 0.05555555555555555),\n              ('and', 0.05555555555555555),\n              ('as', 0),\n              ('faster', 0.05555555555555555),\n              ('get', 0),\n              ('got', 0),\n              ('hairy', 0.05555555555555555),\n              ('harry', 0.05555555555555555),\n              ('home', 0),\n              ('is', 0.05555555555555555),\n              ('jill', 0.05555555555555555),\n              ('not', 0),\n              ('store', 0),\n              ('than', 0.05555555555555555),\n              ('the', 0),\n              ('to', 0),\n              ('would', 0)]),\n OrderedDict([(',', 0),\n              ('.', 0.05555555555555555),\n              ('and', 0),\n              ('as', 0.1111111111111111),\n              ('faster', 0),\n              ('get', 0),\n              ('got', 0),\n              ('hairy', 0.05555555555555555),\n              ('harry', 0.05555555555555555),\n              ('home', 0),\n              ('is', 0.05555555555555555),\n              ('jill', 0.05555555555555555),\n              ('not', 0.05555555555555555),\n              ('store', 0),\n              ('than', 0),\n              ('the', 0),\n              ('to', 0),\n              ('would', 0)])]"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a coarse way to make word (count) vectors for each document in python using built-ins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path().home()/'Desktop'/'nlp-map-project'/'chp3-nlpia-notes'/'img-vects'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/NLPIA-vect-2D.png\" alt=\"Vectors in 2D space\" width=\"400\" height='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1\n",
    "<br>\n",
    "*NLPIA* Lane, Howard and Hapke (2019) chp 3.2.1 pp. 234 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are the foundation of Linear Algebra and such associated computations\n",
    "* *Space* - collection of all possible vectors that could appear in such space representation\n",
    "* Representation - ordered list of numbers/coordinates in a vector space, that are *rectilinear/Euclidean*\n",
    "* Association - they illustrate a location or position in that space or they are used to represent direction and magnitude/distance\n",
    "* Dimensions - A 2D space as in Figure 1 shows vectors with two values, where similar logic for a 3D space would mean vectors with three values etc.\n",
    "<br>\n",
    "<br>\n",
    "Bear in mind that some vectors with 2D values ***cannot*** be represented in a normal 2D space as above such as Geospatial coordinates representing longitude and latitude. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding a natural language document vector space, the ***dimensionality*** of our vector space is the **count of the number of distinct words that appear in the whole corpus**.\n",
    "<br>\n",
    "Notation for dimensionality can be represented as the following: \n",
    "* TF - The dimensionality can be represented as **\"K\"**\n",
    "* Distinct words/lexicon - Represented as the vocabulary size of the corpus, academic papers denote this with **\"|V|\"**\n",
    "<br>\n",
    "It's easy from then on to describe each document within such **K-dimensional vector space** by a **K-dimensional vector** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our simple example of `doc_vectors`, K = 18 in our three document corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img-vects/NLPIA-tf-freq-angle-vects.png\" alt=\"term frequency word vectors and corresponding angles in 2D space\" width=\"400\" height='400'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2\n",
    "<br>\n",
    "*NLPIA* Lane, Howard and Hapke (2019) chp 3.2.1 pp. 238 Apple iBooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the example more easy to visualize, as seen in figure 2, **K** is reduced to two (given 2D view) from the 18 dimensional vector space constructed from our original lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity** - Two vectors are similar when they share similar direction\n",
    "<br>\n",
    "**Magnitude** - represents the \"length\" of the vector, which would mean that two vectors having similar length translates to word count (TF) vectors possessing  the same length\n",
    "<br>\n",
    "<br>\n",
    "The estimate of document similarity provides us with searching the use of the same words about the same number of times in similar proportions.\n",
    "<br>\n",
    "Such metric would provide us with being familiar of the documents they represent as being more likely talking about similar things/topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$cos (\\Theta$) = $\\frac{A \\times B}{|A| \\times |B|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cosine similarity** - Given the above formula, the $\\theta$ represents the cosine of the angle between two vectors (A and B) as a solvable parameter, indicates how 'similar' TF across different documents appear.\n",
    "<br>\n",
    "It then becomes normalized by the Euclidean product (|A|*|B|) with the numerator being the dot product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interpreting output - For most ML problems, cosine similarity has a convenient range in terms of output from -1 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing cosine similarity using numpy \n",
    "import numpy as np \n",
    "def cos_sim(vect_a, vect_b):\n",
    "    return vect_a.dot(vect_a, vect_b) / np.linalg.norm(vect_a) * np.linalg.norm(vect_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing cosine similarity using pure python via the built-in 'math' module\n",
    "import math \n",
    "def cos_sim(vect_a, vect_b):\n",
    "    \"\"\" Convert from dictionary to lists for easier computation and matching \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basis of computing cosine similarity programmatically is to take the dot product of two of our vectors by multiplying the elements of each vector pairwise and sum up those products afterwards.\n",
    "<br>\n",
    "Then divide by the norm (magnitude) of each vector - which again is also the Euclidean distance going from head to tail of each vector as seen in figure 2.\n",
    "* ***normalized dot product*** - This would be an output in a given range between -1 and +1 as with the cosine of the angle between the two vectors \n",
    "* Value illustration - How much the vectors point in the same direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cosine similarity of 1:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine similarity value of 1 illustrates identical normalized vectors that point in identically the same direction along all dimensions. The vectors may have different lengths/magnitudes, but they point in the same direction. \n",
    "<br>\n",
    "<br>\n",
    "It goes with saying that the closer a cosine similarity value is to 1, would also mean that the closer the two values are in angular terms. \n",
    "<br>\n",
    "This would also mean that for NLP document vectors, the documents are using similar words in similar proportions.\n",
    "<br>\n",
    "Hence, documents corresponding to document vectors that are close to each other are likely talking about the same thing/topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cosine similarity of 0:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine similarity value of 0 represents two vectors that share no components whatsoever. They are orthogonal/perpendicular in all dimensions (relative to the origin). \n",
    "<br>\n",
    "With respect to NLP TF vectors, this occurs when two documents (corresponding the terms) share no words in common.\n",
    "<br>\n",
    "<br>\n",
    "Given these documents use absolutely disparate words, they must be discussing talking about different things.\n",
    "<br>\n",
    "Albeit, this definitely doesn't mean they have different *meanings or topics*, it's just referring to the concept that they use different words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cosine similarity of -1:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cosine similarity value of -1 represents two vectors that are completely nonsimilar (opposite) - as they point in opposite directions. This is not the case for a simple word count (term frequency) vectors as well as normalized TF vectors.\n",
    "<br>\n",
    "Hence, word count vectors will always be in the same quadrant of the vector space and term frequency vectors cannot be negative."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}