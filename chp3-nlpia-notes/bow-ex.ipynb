{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending from simple keyword retrieval from tokenization, it might be more useful to understand which words are more 'important' to a particular document and across the corpus as a whole.\n",
    "<br>\n",
    "From this, we can use the 'importance' value to find relevant documents in a corpus based on keyword importance within each document.\n",
    "* Measure positivity relative to tokens across a corpus - Understanding the frequency with which those words appear in a document *in relation* to the rest of the documents, you can use that to further refine the 'positivity' of the document. Hence, learn more about an approach that uses less binary measure of words and their usage within a document - where common use cases revoling around generating features from natural language involves search engines and spam filters\n",
    "* Convert tokens into continous numbers (as opposed to integers representing words counts or binary vectors representing presence/absenc of words) - Representing (transforming) words into a continous form allows more handy computational ability (math), with the the goal of finding the numerical representation of words that capture the importance/information content of the words they represent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three powerful ways to represent words and their importance in a document:\n",
    "<br>\n",
    "<br>\n",
    "1) Bag of words (BOW) - Vectors of words counts/frequencies\n",
    "<br>\n",
    "2) Bag of n-grams - Counts of word pairs (bigrams), triplets (trigrams) etc.\n",
    "<br>\n",
    "3) Term Frequency Inverse Document Frequency (TF-IDF) vectors - Word scores that more so better represent their importance; the technique here is to aggregate the word counts and *divide* each by the number of documents in which the word occurs\n",
    "<br>\n",
    "<br>\n",
    "These techniques are all stastical models in that they are frequency based."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common example that involves counting occurrences of words as a use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"The faster Shuaib got to the store, the faster Shuaib would be able to return home\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['the',\n 'faster',\n 'shuaib',\n 'got',\n 'to',\n 'the',\n 'store',\n ',',\n 'the',\n 'faster',\n 'shuaib',\n 'would',\n 'be',\n 'able',\n 'to',\n 'return',\n 'home']"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Counter({'the': 3,\n         'faster': 2,\n         'shuaib': 2,\n         'got': 1,\n         'to': 2,\n         'store': 1,\n         ',': 1,\n         'would': 1,\n         'be': 1,\n         'able': 1,\n         'return': 1,\n         'home': 1})"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bow = Counter(tokens)\n",
    "bow # Distinct word counts - dictionary like format with no inherent order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "collections.Counter"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "type(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `collections.Counter` object is an unordered collection, also called a bag or multiset. A Counter can be displayed in a seemingly reasonable order, like lexical order or the order that tokens appeared in your statement. But just as for a standard Python `dict`, one can't rely on the order of your tokens (keys) in a Counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BOW contain relevant information about the original intent of the sentence.\n",
    "<br>\n",
    "The information in a BOW can perform meaningful computations such as detecting spam, compute sentiment and subtle intent (sarcasm).\n",
    "<br>\n",
    "Using the `most_common` method, we can find the most frequent (descending order) ranked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('the', 3), ('faster', 2), ('shuaib', 2), ('to', 2)]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# 4 most frequent words \n",
    "bow.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Term Frequency (TF)* - Illustrates the number of times a word occurs in a given document.\n",
    "* TFIDF is where the count of word occurrences are normalized by the number of terms in the document/corupus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this is a rough BOW process - ignore the stop words/prepositions that have no meaning and consider 'faster' and 'shuaib' for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuaib_appears = bow['shuaib']\n",
    "lexicon = len(bow) # number of unique tokens from original document/sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.167"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tf = shuaib_appears/lexicon\n",
    "round(tf, 3) # round to 3 dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw words counts sometimes can be less useful if we want to understand the relevancy of a key word relative to the document/corpus. \n",
    "<br>\n",
    "normalized term frequencies helps us understand the relationship between a specifc term corresponding do a certain document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.data.loaders import kite_text \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kite_text` comprises of paragraphs in the wikipedia article on 'kites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('the', 26), ('a', 20), ('kite', 16), (',', 15), ('and', 10)]"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's highly unlikely the article is talking about stopwords like 'the' and 'and' so we can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for x in tokens if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('kite', 16),\n (',', 15),\n ('kites', 8),\n ('wing', 5),\n ('lift', 4),\n ('may', 4),\n ('also', 3),\n ('kiting', 3)]"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# 8 most common tokens in the corpus\n",
    "kite_counts.most_common(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly see that terms 'kite(s)', 'wing' and 'lift' are of some importance to this document - also allows us to make a decent inference about what the general topic about the document is.\n",
    "<br>\n",
    "An additional application of these term frequencies across multiple documents in a (kite) corpus where the terms 'wing' and 'lift' would rank highly in most of these documents as they are referenced quite often.\n",
    "<br>\n",
    "To show this neatly in mathematical terms, we'll have to apply **vectorization**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37464bitbaseconda572c238c43e2438fb8a17e4e11af0079",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}