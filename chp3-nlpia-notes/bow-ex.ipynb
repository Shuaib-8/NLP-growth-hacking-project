{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending from simple keyword retrieval from tokenization, it might be more useful to understand which words are more 'important' to a particular document and across the corpus as a whole.\n",
    "<br>\n",
    "From this, we can use the 'importance' value to find relevant documents in a corpus based on keyword importance within each document.\n",
    "* Measure positivity relative to tokens across a corpus - Understanding the frequency with which those words appear in a document *in relation* to the rest of the documents, you can use that to further refine the 'positivity' of the document. Hence, learn more about an approach that uses less binary measure of words and their usage within a document - where common use cases revoling around generating features from natural language involves search engines and spam filters\n",
    "* Convert tokens into continous numbers (as opposed to integers representing words counts or binary vectors representing presence/absenc of words) - Representing (transforming) words into a continous form allows more handy computational ability (math), with the the goal of finding the numerical representation of words that capture the importance/information content of the words they represent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three powerful ways to represent words and their importance in a document:\n",
    "<br>\n",
    "<br>\n",
    "1) Bag of words (BOW) - Vectors of words counts/frequencies\n",
    "<br>\n",
    "2) Bag of n-grams - Counts of word pairs (bigrams), triplets (trigrams) etc.\n",
    "<br>\n",
    "3) Term Frequency Inverse Document Frequency (TF-IDF) vectors - Word scores that more so better represent their importance; the technique here is to aggregate the word counts and *divide* each by the number of documents in which the word occurs\n",
    "<br>\n",
    "These techniques are all stastical models in that they are frequency based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}